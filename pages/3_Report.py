"""
ğŸ“ CIO æ—¥æŠ¥ - FOF é…ç½®å†³ç­–ä¸­å¿ƒ
================================================================
å‡çº§ V3: ä½¿ç”¨æ•°æ®æ‰“åŒ… + FOF ä¸“ä¸š Prompt + ç»“æ„åŒ–é…ç½®è¾“å‡º
================================================================
"""
import streamlit as st
import json
import os
from datetime import datetime
import sys

sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from utils.data_fetcher import (
    get_daily_data_pack, pack_market_text, pack_news_text, _tushare_available,
)
from utils.ai_analyzer import generate_daily_report

st.set_page_config(page_title="CIO æ—¥æŠ¥", page_icon="ğŸ“", layout="wide")
st.title("ğŸ“ å¯»æ˜Ÿ CIO æ—¥æŠ¥")
st.caption("AI ç»¼åˆåˆ†æ Â· å¤§ç±»é…ç½® Â· FOF ç­–ç•¥æƒé‡ Â· è¡Œä¸šæ–¹å‘ Â· ä¸ªè‚¡çº¿ç´¢")
st.divider()

# ============================================================
# çŠ¶æ€æ£€æŸ¥
# ============================================================
api_key = ""
try:
    api_key = st.secrets.get("DEEPSEEK_API_KEY", "")
except Exception:
    pass
has_api = bool(api_key and not api_key.startswith("sk-xxxx"))
has_tushare = _tushare_available()

col_s1, col_s2 = st.columns(2)
with col_s1:
    if has_api:
        st.success("ğŸ¤– DeepSeek AI: å·²è¿æ¥")
    else:
        st.error("ğŸ¤– DeepSeek AI: æœªé…ç½®")
with col_s2:
    if has_tushare:
        st.success("ğŸ“¡ Tushare PRO: å·²è¿æ¥")
    else:
        st.warning("ğŸ“¡ Tushare PRO: æœªé…ç½® (èµ„è®¯å°†ä½¿ç”¨æ–°æµªé™çº§æº)")

if not has_api:
    st.warning("""
âš ï¸ **æœªé…ç½® DeepSeek API Key** â€” ç ”æŠ¥éœ€è¦ AI èƒ½åŠ›

1. æ³¨å†Œ [platform.deepseek.com](https://platform.deepseek.com/)
2. åˆ›å»º API Keyï¼Œå……å€¼10å…ƒ
3. Streamlit Cloud â†’ Settings â†’ Secrets æ·»åŠ ï¼š
```
DEEPSEEK_API_KEY = "sk-ä½ çš„å¯†é’¥"
```
    """)

st.divider()

# ============================================================
# ç¼“å­˜
# ============================================================
DATA_DIR = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), "data")
os.makedirs(DATA_DIR, exist_ok=True)
today_file = os.path.join(DATA_DIR, f"report_{datetime.now().strftime('%Y%m%d')}.json")


def load_cache():
    if os.path.exists(today_file):
        with open(today_file, "r", encoding="utf-8") as f:
            return json.load(f)
    return None


def save_cache(data):
    with open(today_file, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2, default=str)


cached = load_cache()
if cached:
    st.info(f"ğŸ“„ å·²æœ‰ä»Šæ—¥ç¼“å­˜æŠ¥å‘Šï¼ˆ{cached.get('time', '')}ï¼‰Â· æ•°æ®æº: {cached.get('data_sources', 'unknown')}")

# ============================================================
# æ“ä½œæŒ‰é’®
# ============================================================
c1, c2 = st.columns(2)
with c1:
    gen_btn = st.button("ğŸš€ ç”Ÿæˆæ–°æŠ¥å‘Š", type="primary", use_container_width=True, disabled=not has_api)
with c2:
    load_btn = st.button("ğŸ“„ æŸ¥çœ‹ç¼“å­˜æŠ¥å‘Š", use_container_width=True, disabled=not cached)

report = None

# ============================================================
# ç”ŸæˆæŠ¥å‘Š
# ============================================================
if gen_btn and has_api:
    progress = st.progress(0, "å‡†å¤‡ä¸­...")

    # Stage 1: å…¨é‡æ•°æ®é‡‡é›†
    progress.progress(10, "ğŸ“¡ é‡‡é›†è¡Œæƒ… + å®è§‚ + èµ„é‡‘æµå‘ + æœŸè´§...")
    data_pack = get_daily_data_pack()

    # æ˜¾ç¤ºé‡‡é›†çŠ¶æ€
    sources_status = []
    if data_pack.get("indices") is not None and not data_pack["indices"].empty:
        sources_status.append("âœ…æŒ‡æ•°")
    if data_pack.get("overview"):
        sources_status.append("âœ…æ¶¨è·Œ")
    if data_pack.get("macro"):
        sources_status.append("âœ…å®è§‚")
    if data_pack.get("northbound"):
        sources_status.append("âœ…åŒ—å‘")
    if data_pack.get("margin"):
        sources_status.append("âœ…èèµ„")
    if data_pack.get("futures"):
        sources_status.append("âœ…æœŸè´§")
    if data_pack.get("news"):
        sources_status.append(f"âœ…èµ„è®¯({len(data_pack['news'])}æ¡)")
    if data_pack.get("research"):
        sources_status.append(f"âœ…ç ”æŠ¥({len(data_pack['research'])}æ¡)")

    progress.progress(40, f"ğŸ“Š æ•°æ®é‡‡é›†å®Œæˆ: {' '.join(sources_status)}")

    # Stage 2: æ•°æ®æ‰“åŒ…ä¸ºæ–‡æœ¬
    progress.progress(50, "ğŸ“¦ æ•°æ®æ‰“åŒ…...")
    market_text = pack_market_text(data_pack)
    news_text = pack_news_text(data_pack)

    # Stage 3: AI ç”ŸæˆæŠ¥å‘Š
    progress.progress(60, "ğŸ¤– DeepSeek æ­£åœ¨ç”Ÿæˆ CIO é…ç½®æŠ¥å‘Š...")
    report = generate_daily_report(market_text, news_text)

    # Stage 4: ä¿å­˜
    progress.progress(90, "ğŸ’¾ ä¿å­˜...")
    data_sources = "Tushare+AKShare+æ–°æµª" if has_tushare else "AKShare+æ–°æµª"
    save_cache({
        "time": datetime.now().strftime("%H:%M"),
        "report": report,
        "data_sources": data_sources,
        "market_text_preview": market_text[:500],
        "news_count": len(data_pack.get("news", [])),
        "research_count": len(data_pack.get("research", [])),
    })

    progress.progress(100, "âœ… å®Œæˆ!")
    st.balloons()

elif load_btn and cached:
    report = cached.get("report", "")

# ============================================================
# å±•ç¤ºæŠ¥å‘Š
# ============================================================
if report:
    st.divider()

    # æŠ¥å‘Šå¤´
    st.markdown(f"""
<div style="padding:16px 20px; border-radius:10px;
background: linear-gradient(135deg, rgba(255,107,53,0.12), rgba(69,183,209,0.06));
border: 1px solid rgba(255,107,53,0.25); margin-bottom:20px;">
<h2 style="margin:0; color:#FF6B35;">ğŸ”­ å¯»æ˜Ÿ FOF CIO æ—¥æŠ¥</h2>
<p style="margin:4px 0 0; color:#999;">{datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥')} Â· DeepSeek V3 Â· æ•°æ®æº: {'Tushare PRO + AKShare' if has_tushare else 'AKShare + æ–°æµª'}</p>
</div>""", unsafe_allow_html=True)

    # æŠ¥å‘Šæ­£æ–‡
    st.markdown(report)

    st.divider()

    # ä¸‹è½½
    c1, c2 = st.columns(2)
    with c1:
        st.download_button(
            "ğŸ“„ ä¸‹è½½ Markdown",
            report,
            f"å¯»æ˜ŸCIOæ—¥æŠ¥_{datetime.now().strftime('%Y%m%d')}.md",
            "text/markdown",
            use_container_width=True,
        )
    with c2:
        st.download_button(
            "ğŸ“ ä¸‹è½½ TXT",
            report.replace("###", "").replace("**", ""),
            f"å¯»æ˜ŸCIOæ—¥æŠ¥_{datetime.now().strftime('%Y%m%d')}.txt",
            "text/plain",
            use_container_width=True,
        )

    # æ•°æ®è¯Šæ–­æŠ˜å é¢æ¿
    with st.expander("ğŸ” æœ¬æ¬¡æŠ¥å‘Šæ•°æ®è¯Šæ–­"):
        if cached:
            st.markdown(f"- èµ„è®¯æ¡æ•°: {cached.get('news_count', '?')}")
            st.markdown(f"- ç ”æŠ¥æ¡æ•°: {cached.get('research_count', '?')}")
            st.markdown(f"- æ•°æ®æº: {cached.get('data_sources', '?')}")
            if cached.get("market_text_preview"):
                st.text(cached["market_text_preview"])

else:
    if not gen_btn:
        st.markdown("""
### ğŸ“‹ æŠ¥å‘ŠåŒ…å«ä»¥ä¸‹å†³ç­–æ¨¡å—

| æ¨¡å— | å†…å®¹ | å¯¹åº”ä½ çš„éœ€æ±‚ |
|------|------|-------------|
| **ä¸€ã€å®è§‚å‘¨æœŸåˆ¤æ–­** | å¤è‹/è¿‡çƒ­/æ»èƒ€/è¡°é€€å®šæ€§ | å¤§ç±»èµ„äº§æ–¹å‘çš„ç†è®ºåŸºç¡€ |
| **äºŒã€å¤§ç±»èµ„äº§é…ç½®** | æƒç›Š/å›ºæ”¶/å•†å“/ç°é‡‘å…·ä½“æ¯”ä¾‹ | è‚¡ç¥¨ã€å€ºåˆ¸ã€å•†å“çš„é…ç½®æƒé‡ |
| **ä¸‰ã€FOFç­–ç•¥é…ç½®** | 7é¡¹ç­–ç•¥å…·ä½“æ¯”ä¾‹(åˆè®¡100%) | å¤šå¤´/æŒ‡å¢/ä¸­æ€§/CTA/å¥—åˆ©/å›ºæ”¶+ |
| **å››ã€é£æ ¼ä¸è¡Œä¸š** | å¤§å°ç›˜+æˆé•¿ä»·å€¼+TOP3è¡Œä¸š | å¸‚åœºé£æ ¼æ–¹å‘+è¡Œä¸šæ–¹å‘ |
| **äº”ã€æˆ˜æœ¯å·¥å…·ç®±** | ETFä»£ç +ä¸ªè‚¡çº¿ç´¢ | å…·ä½“çš„æ‰§è¡Œå·¥å…· |
| **å…­ã€é£é™©é¢„è­¦** | 3å¤§é£é™©+å¯¹å†²å»ºè®® | é˜²å¾¡é…ç½®å’Œå°¾éƒ¨é£é™©ç®¡ç† |

ğŸ‘† ç‚¹å‡» **ã€Œç”Ÿæˆæ–°æŠ¥å‘Šã€** å¼€å§‹

---

**ğŸ’¡ æ•°æ®æºè¯´æ˜**:
- **Tushare PRO** (å·²é…ç½®âœ…): è´¢ç»æ–°é—»ã€æ–°é—»è”æ’­ã€åˆ¸å•†ç ”æŠ¥è¯„çº§ã€èèµ„èåˆ¸
- **AKShare** (å…è´¹): Aè‚¡è¡Œæƒ…ã€æŒ‡æ•°ã€æ¿å—ã€ETFã€å®è§‚æ•°æ®ã€åŒ—å‘èµ„é‡‘ã€æœŸè´§
- **æ–°æµªå¿«è®¯** (è¡¥å……): ç›˜ä¸­å¼‚åŠ¨å®æ—¶å¿«è®¯
        """)

st.divider()
st.caption(f"å¯»æ˜Ÿé…ç½®è·Ÿè¸ªç³»ç»Ÿ Â· v3.0 Â· {datetime.now().strftime('%H:%M:%S')}")
