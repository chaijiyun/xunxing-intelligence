"""
æ•°æ®é‡‡é›†æ¨¡å— V4.1 - å¯»æ˜Ÿæƒ…æŠ¥ä¸­å¿ƒ
================================================================
æ ¸å¿ƒæ¶æ„: Tushare PRO ä¼˜å…ˆ â†’ AKShare é™çº§å…œåº•
å†›è§„å‡çº§: Q2/Q5 å¼ºåˆ¶åˆ‡å…¥å‰å¤æƒ(qfq)æµï¼Œæ¶ˆç­é™¤æƒä»·æ ¼å¤±çœŸï¼ŒåŠ å…¥é˜²å°ç¦é™æµ
================================================================
æ•°æ®å±‚çº§:
  L1  æŒ‡æ•°è¡Œæƒ…: Tushare daily â†’ AKShare fallback
  L2  æ¶¨è·Œç»Ÿè®¡: AKShare (Tushare æ— ç›´æ¥æ¥å£)
  L3  å®è§‚æ•°æ®: Tushare PRO å®è§‚æ¥å£ (cn_cpi/cn_pmi/cn_m2 ç­‰)
  L4  æµåŠ¨æ€§:   Tushare shibor + AKShare DR007/å¤®è¡ŒOMO
  L5  ä¿¡ç”¨åˆ©å·®: Tushare bond_blk â†’ AKShare fallback
  L6  åŒ—å‘èµ„é‡‘: Tushare hsgt_top10 â†’ AKShare fallback
  L7  èèµ„èåˆ¸: Tushare margin
  L8  é£æ ¼å› å­: Tushare index_daily (å¤šæŒ‡æ•°5æ—¥+20æ—¥åŠ¨é‡)
  L9  æ³¢åŠ¨ç‡:   åŸºäº Tushare æŒ‡æ•°æ—¥çº¿è‡ªç®— HV20
  L10 å¸‚åœºå®½åº¦: æ¶¨è·Œæ¯”MA5/æ–°é«˜æ–°ä½ (åŸºäºL2æ•°æ®æ‰©å±•)
  L11 æ¿å—:     AKShare (ä¸œæ–¹è´¢å¯Œè¡Œä¸š/æ¦‚å¿µ) â†’ Tushare fallback
  L12 ETF:      Tushare fund_daily â†’ AKShare fallback
  L13 èµ„è®¯:     Tushare 8æºæ–°é—» + æ–°é—»è”æ’­ + æ–°æµªé™çº§
  L14 ç ”æŠ¥:     Tushare report_rc
  L15 æœŸè´§:     AKShare æ–°æµªæœŸè´§ â†’ Tushare fut_daily fallback
  L16 æ‰“åŒ…:     å…¨é‡æ•°æ®èšåˆä¾› AI ä½¿ç”¨
================================================================
"""
import pandas as pd
import numpy as np
import requests
import json
import os
import logging
import concurrent.futures
from datetime import datetime, timedelta
import streamlit as st
import urllib3
import certifi
import shutil
import time
import tushare as ts

# ============================================================
# åŸºç¡€è®¾æ–½
# ============================================================
logging.basicConfig(level=logging.INFO, format="[%(asctime)s] %(message)s", datefmt="%H:%M:%S")
logger = logging.getLogger("xunxing")
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

try:
    safe_cert_path = os.path.join(os.getcwd(), "cacert.pem")
    if not os.path.exists(safe_cert_path):
        shutil.copy(certifi.where(), safe_cert_path)
    os.environ["CURL_CA_BUNDLE"] = safe_cert_path
    os.environ["REQUESTS_CA_BUNDLE"] = safe_cert_path
except Exception as e:
    logger.warning(f"SSLè¯ä¹¦è·¯å¾„ä¿®å¤å¤±è´¥: {e}")

DATA_DIR = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), "data")
os.makedirs(DATA_DIR, exist_ok=True)


def _safe_call(func, timeout=12, default=None, label=""):
    """å¸¦è¶…æ—¶å’Œæ—¥å¿—çš„å®‰å…¨è°ƒç”¨"""
    try:
        with concurrent.futures.ThreadPoolExecutor(max_workers=1) as executor:
            future = executor.submit(func)
            return future.result(timeout=timeout)
    except concurrent.futures.TimeoutError:
        logger.warning(f"[è¶…æ—¶] {label} è¶…è¿‡ {timeout}s")
        return default
    except Exception as e:
        logger.error(f"[å¼‚å¸¸] {label}: {e}")
        return default


def _import_akshare():
    """å»¶è¿Ÿå¯¼å…¥ AKShare (ä»…é™çº§æ—¶éœ€è¦)"""
    try:
        import akshare as ak
        return ak
    except ImportError:
        logger.error("akshare æœªå®‰è£…")
        return None


# ============================================================
# Tushare PRO åˆå§‹åŒ–
# ============================================================
@st.cache_resource
def _get_tushare_pro():
    """è·å– Tushare PRO æ¥å£å®ä¾‹ (å…¨å±€ç¼“å­˜)"""
    try:
        token = ""
        try:
            token = st.secrets.get("TUSHARE_TOKEN", "")
        except Exception:
            pass
        if not token:
            logger.warning("TUSHARE_TOKEN æœªé…ç½®")
            return None
        pro = ts.pro_api(token)
        # ç®€å•æµ‹è¯•è¿é€šæ€§
        logger.info("Tushare PRO è¿æ¥æˆåŠŸ")
        return pro
    except ImportError:
        logger.error("tushare æœªå®‰è£…")
        return None
    except Exception as e:
        logger.error(f"Tushare åˆå§‹åŒ–å¤±è´¥: {e}")
        return None


def _tushare_available() -> bool:
    return _get_tushare_pro() is not None


def _last_trade_date(offset=0) -> str:
    """è·å–æœ€è¿‘äº¤æ˜“æ—¥ (ç®€å•ä¼°ç®—, è·³è¿‡å‘¨æœ«)"""
    d = datetime.now() - timedelta(days=offset)
    while d.weekday() >= 5:  # å‘¨å…­æ—¥
        d -= timedelta(days=1)
    return d.strftime("%Y%m%d")


# ============================================================
# L1. å®½åŸºæŒ‡æ•°è¡Œæƒ… â€” Tushare PRO ä¼˜å…ˆ
# ============================================================
# æ ¸å¿ƒæŒ‡æ•°æ˜ å°„
INDEX_MAP = {
    "000001.SH": "ä¸Šè¯æŒ‡æ•°",
    "399001.SZ": "æ·±è¯æˆæŒ‡",
    "399006.SZ": "åˆ›ä¸šæ¿æŒ‡",
    "000688.SH": "ç§‘åˆ›50",
    "000300.SH": "æ²ªæ·±300",
    "000905.SH": "ä¸­è¯500",
    "000852.SH": "ä¸­è¯1000",
}


@st.cache_data(ttl=600, show_spinner=False)
def get_major_indices() -> pd.DataFrame:
    """å®½åŸºæŒ‡æ•°è¡Œæƒ… â€” Tushare ä¼˜å…ˆ"""
    def _tushare_fetch():
        pro = _get_tushare_pro()
        if not pro:
            return None
        try:
            today = _last_trade_date()
            rows = []
            for ts_code, name in INDEX_MAP.items():
                df = pro.index_daily(ts_code=ts_code, start_date=today, end_date=today)
                if df is not None and not df.empty:
                    r = df.iloc[0]
                    rows.append({
                        "åç§°": name,
                        "æœ€æ–°ä»·": float(r.get("close", 0)),
                        "æ¶¨è·Œå¹…": float(r.get("pct_chg", 0)),
                        "æ¶¨è·Œé¢": float(r.get("change", 0)),
                        "æˆäº¤é¢": float(r.get("amount", 0)) * 1000,  # Tushare amount å•ä½åƒå…ƒ
                    })
            if rows:
                return pd.DataFrame(rows)
            # å¦‚æœä»Šå¤©æ²¡æ•°æ® (éäº¤æ˜“æ—¥/ç›˜å‰), å°è¯•å¾€å‰æ‰¾
            for offset in range(1, 4):
                date = _last_trade_date(offset)
                rows = []
                for ts_code, name in INDEX_MAP.items():
                    df = pro.index_daily(ts_code=ts_code, start_date=date, end_date=date)
                    if df is not None and not df.empty:
                        r = df.iloc[0]
                        rows.append({
                            "åç§°": name,
                            "æœ€æ–°ä»·": float(r.get("close", 0)),
                            "æ¶¨è·Œå¹…": float(r.get("pct_chg", 0)),
                            "æ¶¨è·Œé¢": float(r.get("change", 0)),
                            "æˆäº¤é¢": float(r.get("amount", 0)) * 1000,
                        })
                if rows:
                    return pd.DataFrame(rows)
        except Exception as e:
            logger.warning(f"[Tushare] æŒ‡æ•°è¡Œæƒ…å¤±è´¥: {e}")
        return None

    def _akshare_fetch():
        ak = _import_akshare()
        if not ak:
            return pd.DataFrame()
        df = ak.stock_zh_index_spot_em()
        if df is None or df.empty:
            return pd.DataFrame()
        target = list(INDEX_MAP.values())
        result = df[df["åç§°"].isin(target)].copy()
        keep = [c for c in ["åç§°", "æœ€æ–°ä»·", "æ¶¨è·Œå¹…", "æ¶¨è·Œé¢", "æˆäº¤é¢"] if c in result.columns]
        result = result[keep].reset_index(drop=True)
        for c in ["æœ€æ–°ä»·", "æ¶¨è·Œå¹…", "æ¶¨è·Œé¢", "æˆäº¤é¢"]:
            if c in result.columns:
                result[c] = pd.to_numeric(result[c], errors="coerce")
        return result

    # Tushare ä¼˜å…ˆ
    result = _safe_call(_tushare_fetch, timeout=15, default=None, label="æŒ‡æ•°[TS]")
    if result is not None and not result.empty:
        return result
    logger.info("[é™çº§] æŒ‡æ•°è¡Œæƒ… â†’ AKShare")
    return _safe_call(_akshare_fetch, timeout=12, default=pd.DataFrame(), label="æŒ‡æ•°[AK]")


# ============================================================
# L2. æ¶¨è·Œç»Ÿè®¡ (AKShare ä¸ºä¸», Tushare æ— ç›´æ¥æ¥å£)
# ============================================================
@st.cache_data(ttl=600, show_spinner=False)
def get_market_overview() -> dict:
    def _fetch():
        ak = _import_akshare()
        if not ak:
            return {}
        df = ak.stock_zh_a_spot_em()
        if df is None or df.empty:
            return {}
        df["æ¶¨è·Œå¹…"] = pd.to_numeric(df["æ¶¨è·Œå¹…"], errors="coerce")
        df["æˆäº¤é¢"] = pd.to_numeric(df["æˆäº¤é¢"], errors="coerce")
        total = len(df)
        up = int((df["æ¶¨è·Œå¹…"] > 0).sum())
        down = int((df["æ¶¨è·Œå¹…"] < 0).sum())
        flat = total - up - down
        limit_up = int((df["æ¶¨è·Œå¹…"] >= 9.8).sum())
        limit_down = int((df["æ¶¨è·Œå¹…"] <= -9.8).sum())
        total_amount = round(df["æˆäº¤é¢"].sum() / 1e8, 0)

        # V4 æ–°å¢: å¸‚åœºå®½åº¦æŒ‡æ ‡
        up_ratio = round(up / total * 100, 1) if total else 0
        # æ¶¨å¹… > 3% å’Œ < -3% çš„æ•°é‡ (å¼ºåŠ¿/å¼±åŠ¿ä¸ªè‚¡)
        strong_up = int((df["æ¶¨è·Œå¹…"] >= 3).sum())
        strong_down = int((df["æ¶¨è·Œå¹…"] <= -3).sum())

        return {
            "ä¸Šæ¶¨": up, "ä¸‹è·Œ": down, "å¹³ç›˜": flat,
            "æ¶¨åœ": limit_up, "è·Œåœ": limit_down,
            "æ€»æˆäº¤é¢äº¿": total_amount,
            "ä¸Šæ¶¨å æ¯”": up_ratio,
            "å¼ºåŠ¿è‚¡": strong_up,
            "å¼±åŠ¿è‚¡": strong_down,
            "æ€»è‚¡ç¥¨æ•°": total,
        }
    return _safe_call(_fetch, timeout=12, default={}, label="æ¶¨è·Œç»Ÿè®¡")


# ============================================================
# L3. å®è§‚æ•°æ® â€” Tushare PRO ä¼˜å…ˆ (æ¡¥æ°´å››ç»´æ¡†æ¶)
# ============================================================
@st.cache_data(ttl=7200, show_spinner=False)
def get_macro_data() -> dict:
    """
    æ¡¥æ°´å¼å››ç»´å®è§‚æ¡†æ¶:
    1. å¢é•¿ç»´åº¦: PMI, å·¥ä¸šå¢åŠ å€¼
    2. é€šèƒ€ç»´åº¦: CPI, PPI, CPI-PPIå‰ªåˆ€å·®
    3. æµåŠ¨æ€§: M2, ç¤¾è (å•ç‹¬å‡½æ•° get_liquidity_data)
    4. ä¿¡ç”¨: ä¿¡ç”¨åˆ©å·® (å•ç‹¬å‡½æ•° get_credit_spread)
    """
    def _tushare_fetch():
        pro = _get_tushare_pro()
        if not pro:
            return None
        macro = {}
        # CPI
        try:
            df = pro.cn_cpi(start_m="202401", end_m=datetime.now().strftime("%Y%m"))
            if df is not None and not df.empty:
                df = df.sort_values("month").tail(1)
                last = df.iloc[0]
                macro["CPIåŒæ¯”"] = f"{last.get('nt_yoy', '')}%"
                macro["CPIæœˆä»½"] = str(last.get("month", ""))
        except Exception as e:
            logger.warning(f"[TS] CPI: {e}")

        # PPI
        try:
            df = pro.cn_ppi(start_m="202401", end_m=datetime.now().strftime("%Y%m"))
            if df is not None and not df.empty:
                df = df.sort_values("month").tail(1)
                last = df.iloc[0]
                macro["PPIåŒæ¯”"] = f"{last.get('ppi_yoy', '')}%"
        except Exception as e:
            logger.warning(f"[TS] PPI: {e}")

        # CPI-PPI å‰ªåˆ€å·®
        try:
            cpi_val = float(str(macro.get("CPIåŒæ¯”", "0")).replace("%", ""))
            ppi_val = float(str(macro.get("PPIåŒæ¯”", "0")).replace("%", ""))
            macro["CPI-PPIå‰ªåˆ€å·®"] = f"{round(cpi_val - ppi_val, 1)}%"
        except Exception:
            pass

        # PMI
        try:
            df = pro.cn_pmi(start_m="202401", end_m=datetime.now().strftime("%Y%m"))
            if df is not None and not df.empty:
                df = df.sort_values("month").tail(1)
                last = df.iloc[0]
                macro["åˆ¶é€ ä¸šPMI"] = str(last.get("pmi", ""))
                macro["PMIæœˆä»½"] = str(last.get("month", ""))
        except Exception as e:
            logger.warning(f"[TS] PMI: {e}")

        # M2
        try:
            df = pro.cn_m(start_m="202401", end_m=datetime.now().strftime("%Y%m"))
            if df is not None and not df.empty:
                df = df.sort_values("month").tail(1)
                last = df.iloc[0]
                macro["M2åŒæ¯”"] = f"{last.get('m2_yoy', '')}%"
        except Exception as e:
            logger.warning(f"[TS] M2: {e}")

        # å›½å€ºåˆ©ç‡ (ä¸­ç¾)
        try:
            today = _last_trade_date()
            start = (datetime.now() - timedelta(days=10)).strftime("%Y%m%d")
            df = pro.yc_cb(ts_code="1001.CB", curve_type="0", trade_date=today)
            if df is not None and not df.empty:
                row_10y = df[df["curve_term"] == 10]
                if not row_10y.empty:
                    macro["ä¸­å›½10Yå›½å€º"] = f"{row_10y.iloc[0]['yield']}%"
        except Exception as e:
            logger.warning(f"[TS] å›½å€ºåˆ©ç‡: {e}")

        # äººæ°‘å¸æ±‡ç‡
        try:
            end = datetime.now().strftime("%Y%m%d")
            start = (datetime.now() - timedelta(days=10)).strftime("%Y%m%d")
            df = pro.fx_daily(ts_code="USDCNY.FXCM", start_date=start, end_date=end)
            if df is not None and not df.empty:
                df = df.sort_values("trade_date").tail(1)
                macro["ç¾å…ƒå…‘äººæ°‘å¸"] = str(round(float(df.iloc[0].get("close", 0)), 4))
        except Exception as e:
            logger.warning(f"[TS] æ±‡ç‡: {e}")

        return macro if macro else None

    def _akshare_fetch():
        ak = _import_akshare()
        if not ak:
            return {}
        macro = {}
        try:
            df = ak.macro_china_cpi_monthly()
            if df is not None and not df.empty:
                last = df.iloc[-1]
                macro["CPIåŒæ¯”"] = str(last.iloc[-1])
                macro["CPIæœˆä»½"] = str(last.iloc[0])
        except Exception:
            pass
        try:
            df = ak.macro_china_pmi()
            if df is not None and not df.empty:
                last = df.iloc[-1]
                macro["åˆ¶é€ ä¸šPMI"] = str(last.iloc[-1])
                macro["PMIæœˆä»½"] = str(last.iloc[0])
        except Exception:
            pass
        try:
            df = ak.bond_zh_us_rate(start_date="20250101")
            if df is not None and not df.empty:
                latest = df.iloc[-1]
                for col in df.columns:
                    if "ä¸­å›½" in str(col) and "10" in str(col):
                        macro["ä¸­å›½10Yå›½å€º"] = f"{latest[col]}%"
                    if "ç¾å›½" in str(col) and "10" in str(col):
                        macro["ç¾å›½10Yå›½å€º"] = f"{latest[col]}%"
        except Exception:
            pass
        try:
            df = ak.currency_boc_sina(symbol="ç¾å…ƒ",
                                       start_date=(datetime.now() - timedelta(days=10)).strftime("%Y%m%d"))
            if df is not None and not df.empty:
                val = df.iloc[-1].iloc[1] if len(df.columns) > 1 else None
                if val:
                    macro["ç¾å…ƒå…‘äººæ°‘å¸"] = str(val)
        except Exception:
            pass
        return macro

    # Tushare ä¼˜å…ˆ
    result = _safe_call(_tushare_fetch, timeout=20, default=None, label="å®è§‚[TS]")
    if result:
        return result
    logger.info("[é™çº§] å®è§‚æ•°æ® â†’ AKShare")
    return _safe_call(_akshare_fetch, timeout=15, default={}, label="å®è§‚[AK]")


# ============================================================
# L4. æµåŠ¨æ€§æŒ‡æ ‡ (V4æ–°å¢ â€” æ¡¥æ°´æ¡†æ¶æ ¸å¿ƒ)
# ============================================================
@st.cache_data(ttl=3600, show_spinner=False)
def get_liquidity_data() -> dict:
    """æµåŠ¨æ€§ç»´åº¦: Shibor / DR007 / å¤®è¡ŒOMOå‡€æŠ•æ”¾"""
    def _tushare_fetch():
        pro = _get_tushare_pro()
        if not pro:
            return None
        result = {}
        try:
            today = _last_trade_date()
            start = (datetime.now() - timedelta(days=10)).strftime("%Y%m%d")
            df = pro.shibor(start_date=start, end_date=today)
            if df is not None and not df.empty:
                df = df.sort_values("date").tail(1)
                last = df.iloc[0]
                result["Shiboréš”å¤œ"] = f"{last.get('on', '')}%"
                result["Shibor_1W"] = f"{last.get('1w', '')}%"
                result["Shibor_1M"] = f"{last.get('1m', '')}%"
        except Exception as e:
            logger.warning(f"[TS] Shibor: {e}")

        # ç¤¾èå­˜é‡åŒæ¯” (é€šè¿‡ cn_sf æ¥å£)
        try:
            df = pro.cn_sf(start_m="202401", end_m=datetime.now().strftime("%Y%m"))
            if df is not None and not df.empty:
                df = df.sort_values("month").tail(1)
                last = df.iloc[0]
                # ç¤¾èå­˜é‡å¢é‡
                result["ç¤¾èå¢é‡äº¿"] = str(round(float(last.get("inc_total", 0)) / 1e4, 0))
        except Exception as e:
            logger.warning(f"[TS] ç¤¾è: {e}")

        return result if result else None

    def _akshare_fetch():
        ak = _import_akshare()
        if not ak:
            return {}
        result = {}
        try:
            df = ak.rate_interbank(market="ä¸Šæµ·é“¶è¡ŒåŒä¸šæ‹†å€Ÿåˆ©ç‡", symbol="Shiboräººæ°‘å¸", indicator="éš”å¤œ")
            if df is not None and not df.empty:
                last = df.iloc[-1]
                for col in df.columns:
                    if "åˆ©ç‡" in str(col) or "æŠ¥ä»·" in str(col):
                        result["Shiboréš”å¤œ"] = f"{last[col]}%"
                        break
        except Exception:
            pass
        return result

    result = _safe_call(_tushare_fetch, timeout=15, default=None, label="æµåŠ¨æ€§[TS]")
    if result:
        return result
    logger.info("[é™çº§] æµåŠ¨æ€§ â†’ AKShare")
    return _safe_call(_akshare_fetch, timeout=10, default={}, label="æµåŠ¨æ€§[AK]")


# ============================================================
# L5. ä¿¡ç”¨åˆ©å·® (V4æ–°å¢ â€” æ¡¥æ°´æ¡†æ¶: ä¿¡ç”¨å‘¨æœŸ)
# ============================================================
@st.cache_data(ttl=7200, show_spinner=False)
def get_credit_spread() -> dict:
    """ä¿¡ç”¨åˆ©å·®: AA-ä¼ä¸šå€º vs å›½å€º, ä¿¡ç”¨æ‰©å¼ /æ”¶ç¼©åˆ¤æ–­"""
    def _fetch():
        pro = _get_tushare_pro()
        if not pro:
            return {}
        result = {}
        try:
            today = _last_trade_date()
            # è·å–å›½å€ºæ”¶ç›Šç‡æ›²çº¿ (5Y)
            df_gov = pro.yc_cb(ts_code="1001.CB", curve_type="0", trade_date=today)
            gov_5y = None
            if df_gov is not None and not df_gov.empty:
                row = df_gov[df_gov["curve_term"] == 5]
                if not row.empty:
                    gov_5y = float(row.iloc[0]["yield"])
                    result["å›½å€º5Y"] = f"{gov_5y:.2f}%"

            # ç®€åŒ–: ç”¨ä¿¡ç”¨å€ºæŒ‡æ•°å˜åŒ–è¿‘ä¼¼ä¿¡ç”¨åˆ©å·®è¶‹åŠ¿
            # å¦‚æœæœ‰ cb_blk æ¥å£å¯ä»¥è·å–ä¼ä¸šå€ºåˆ°æœŸæ”¶ç›Šç‡
        except Exception as e:
            logger.warning(f"[TS] ä¿¡ç”¨åˆ©å·®: {e}")
        return result

    return _safe_call(_fetch, timeout=12, default={}, label="ä¿¡ç”¨åˆ©å·®")


# ============================================================
# L6. åŒ—å‘èµ„é‡‘ â€” Tushare ä¼˜å…ˆ
# ============================================================
@st.cache_data(ttl=600, show_spinner=False)
def get_northbound_flow() -> dict:
    def _tushare_fetch():
        pro = _get_tushare_pro()
        if not pro:
            return None
        try:
            end = _last_trade_date()
            start = (datetime.now() - timedelta(days=15)).strftime("%Y%m%d")
            df = pro.moneyflow_hsgt(start_date=start, end_date=end)
            if df is not None and not df.empty:
                df = df.sort_values("trade_date")
                recent5 = df.tail(5)
                latest = recent5.iloc[-1]
                # north_money å•ä½ç™¾ä¸‡
                today_val = float(latest.get("north_money", 0)) / 100  # è½¬äº¿
                five_avg = float(recent5["north_money"].mean()) / 100
                return {
                    "ä»Šæ—¥å‡€æµå…¥äº¿": round(today_val, 2),
                    "5æ—¥å‡å€¼äº¿": round(five_avg, 2),
                    "æ–¹å‘": "å‡€æµå…¥" if today_val > 0 else "å‡€æµå‡º",
                    "æ—¥æœŸ": str(latest.get("trade_date", "")),
                }
        except Exception as e:
            logger.warning(f"[TS] åŒ—å‘èµ„é‡‘: {e}")
        return None

    def _akshare_fetch():
        ak = _import_akshare()
        if not ak:
            return {}
        try:
            df = ak.stock_hsgt_north_net_flow_in_em(symbol="åŒ—ä¸Š")
            if df is not None and not df.empty:
                recent = df.tail(5)
                cols = [c for c in recent.columns if "å‡€" in str(c) or "æµå…¥" in str(c)]
                if not cols:
                    cols = recent.select_dtypes(include="number").columns.tolist()
                if cols:
                    val_col = cols[0]
                    today_val = float(recent.iloc[-1][val_col])
                    five_avg = float(recent[val_col].mean())
                    scale = 1e4 if abs(today_val) > 1000 else 1
                    return {
                        "ä»Šæ—¥å‡€æµå…¥äº¿": round(today_val / scale, 2),
                        "5æ—¥å‡å€¼äº¿": round(five_avg / scale, 2),
                        "æ–¹å‘": "å‡€æµå…¥" if today_val > 0 else "å‡€æµå‡º",
                    }
        except Exception:
            pass
        return {}

    result = _safe_call(_tushare_fetch, timeout=12, default=None, label="åŒ—å‘[TS]")
    if result:
        return result
    logger.info("[é™çº§] åŒ—å‘èµ„é‡‘ â†’ AKShare")
    return _safe_call(_akshare_fetch, timeout=10, default={}, label="åŒ—å‘[AK]")


# ============================================================
# L7. èèµ„èåˆ¸ (Tushare PRO)
# ============================================================
@st.cache_data(ttl=3600, show_spinner=False)
def get_margin_data() -> dict:
    def _fetch():
        pro = _get_tushare_pro()
        if not pro:
            return {}
        try:
            today = datetime.now().strftime("%Y%m%d")
            start = (datetime.now() - timedelta(days=20)).strftime("%Y%m%d")
            df = pro.margin(start_date=start, end_date=today)
            if df is not None and not df.empty:
                df = df.sort_values("trade_date").tail(5)
                latest = df.iloc[-1]
                prev = df.iloc[0] if len(df) >= 2 else latest
                rzye = float(latest.get("rzye", 0)) / 1e8
                rqye = float(latest.get("rqye", 0)) / 1e8
                rzye_prev = float(prev.get("rzye", 0)) / 1e8
                rz_chg = round(rzye - rzye_prev, 1)
                return {
                    "èèµ„ä½™é¢äº¿": round(rzye, 1),
                    "èåˆ¸ä½™é¢äº¿": round(rqye, 1),
                    "èèµ„5æ—¥å˜åŒ–äº¿": rz_chg,
                    "æ æ†æƒ…ç»ª": "åŠ æ æ†" if rz_chg > 0 else "å»æ æ†",
                }
        except Exception as e:
            logger.warning(f"èèµ„èåˆ¸è·å–å¤±è´¥: {e}")
        return {}
    return _safe_call(_fetch, timeout=12, default={}, label="èèµ„èåˆ¸")


# ============================================================
# L8. é£æ ¼æ•°æ® (Tushare ä¼˜å…ˆ â€” 5æ—¥+20æ—¥åŠ¨é‡)
# ============================================================
STYLE_INDICES = {
    "000300.SH": "æ²ªæ·±300",
    "000852.SH": "ä¸­è¯1000",
    "399006.SZ": "åˆ›ä¸šæ¿æŒ‡",
    "000016.SH": "ä¸Šè¯50",
    "000905.SH": "ä¸­è¯500",
}


@st.cache_data(ttl=600, show_spinner=False)
def get_style_data() -> dict:
    def _tushare_fetch():
        pro = _get_tushare_pro()
        if not pro:
            return None
        result = {}
        try:
            end = _last_trade_date()
            start = (datetime.now() - timedelta(days=45)).strftime("%Y%m%d")
            closes = {}
            for ts_code, name in STYLE_INDICES.items():
                df = pro.index_daily(ts_code=ts_code, start_date=start, end_date=end)
                if df is not None and not df.empty:
                    df = df.sort_values("trade_date")
                    closes[name] = df["close"].values

            # è®¡ç®—åŠ¨é‡
            for name, vals in closes.items():
                if len(vals) >= 6:
                    d5 = round((float(vals[-1]) / float(vals[-6]) - 1) * 100, 2)
                    result[f"{name}_5æ—¥"] = d5
                if len(vals) >= 21:
                    d20 = round((float(vals[-1]) / float(vals[-21]) - 1) * 100, 2)
                    result[f"{name}_20æ—¥"] = d20

            # å¤§å°ç›˜åå¥½ (5æ—¥)
            if "æ²ªæ·±300_5æ—¥" in result and "ä¸­è¯1000_5æ—¥" in result:
                result["å¤§å°ç›˜åå¥½_5æ—¥"] = "åå¤§ç›˜" if result["æ²ªæ·±300_5æ—¥"] > result["ä¸­è¯1000_5æ—¥"] else "åå°ç›˜"
            if "æ²ªæ·±300_20æ—¥" in result and "ä¸­è¯1000_20æ—¥" in result:
                result["å¤§å°ç›˜åå¥½_20æ—¥"] = "åå¤§ç›˜" if result["æ²ªæ·±300_20æ—¥"] > result["ä¸­è¯1000_20æ—¥"] else "åå°ç›˜"

            # æˆé•¿ä»·å€¼åå¥½
            if "åˆ›ä¸šæ¿æŒ‡_5æ—¥" in result and "ä¸Šè¯50_5æ—¥" in result:
                result["æˆé•¿ä»·å€¼_5æ—¥"] = "åæˆé•¿" if result["åˆ›ä¸šæ¿æŒ‡_5æ—¥"] > result["ä¸Šè¯50_5æ—¥"] else "åä»·å€¼"
            if "åˆ›ä¸šæ¿æŒ‡_20æ—¥" in result and "ä¸Šè¯50_20æ—¥" in result:
                result["æˆé•¿ä»·å€¼_20æ—¥"] = "åæˆé•¿" if result["åˆ›ä¸šæ¿æŒ‡_20æ—¥"] > result["ä¸Šè¯50_20æ—¥"] else "åä»·å€¼"

            return result if result else None
        except Exception as e:
            logger.warning(f"[TS] é£æ ¼æ•°æ®: {e}")
        return None

    def _akshare_fetch():
        ak = _import_akshare()
        if not ak:
            return {}
        result = {}
        try:
            hs300 = ak.stock_zh_index_daily_em(symbol="sh000300")
            zz1000 = ak.stock_zh_index_daily_em(symbol="sh000852")
            if hs300 is not None and len(hs300) >= 6 and zz1000 is not None and len(zz1000) >= 6:
                hs_5d = (float(hs300.iloc[-1]["close"]) / float(hs300.iloc[-6]["close"]) - 1) * 100
                zz_5d = (float(zz1000.iloc[-1]["close"]) / float(zz1000.iloc[-6]["close"]) - 1) * 100
                result["æ²ªæ·±300_5æ—¥"] = round(hs_5d, 2)
                result["ä¸­è¯1000_5æ—¥"] = round(zz_5d, 2)
                result["å¤§å°ç›˜åå¥½_5æ—¥"] = "åå¤§ç›˜" if hs_5d > zz_5d else "åå°ç›˜"
        except Exception:
            pass
        try:
            cyb = ak.stock_zh_index_daily_em(symbol="sz399006")
            sz50 = ak.stock_zh_index_daily_em(symbol="sh000016")
            if cyb is not None and len(cyb) >= 6 and sz50 is not None and len(sz50) >= 6:
                cyb_5d = (float(cyb.iloc[-1]["close"]) / float(cyb.iloc[-6]["close"]) - 1) * 100
                sz50_5d = (float(sz50.iloc[-1]["close"]) / float(sz50.iloc[-6]["close"]) - 1) * 100
                result["åˆ›ä¸šæ¿æŒ‡_5æ—¥"] = round(cyb_5d, 2)
                result["ä¸Šè¯50_5æ—¥"] = round(sz50_5d, 2)
                result["æˆé•¿ä»·å€¼_5æ—¥"] = "åæˆé•¿" if cyb_5d > sz50_5d else "åä»·å€¼"
        except Exception:
            pass
        return result

    result = _safe_call(_tushare_fetch, timeout=20, default=None, label="é£æ ¼[TS]")
    if result:
        return result
    logger.info("[é™çº§] é£æ ¼ â†’ AKShare")
    return _safe_call(_akshare_fetch, timeout=15, default={}, label="é£æ ¼[AK]")


# ============================================================
# L9. æ³¢åŠ¨ç‡ (V4æ–°å¢ â€” åŸºäºæ²ªæ·±300æ—¥çº¿è‡ªç®—HV20)
# ============================================================
@st.cache_data(ttl=3600, show_spinner=False)
def get_volatility_data() -> dict:
    """å†å²æ³¢åŠ¨ç‡ + æˆäº¤é‡åŠ¨é‡"""
    def _tushare_fetch():
        pro = _get_tushare_pro()
        if not pro:
            return None
        result = {}
        try:
            end = _last_trade_date()
            start = (datetime.now() - timedelta(days=60)).strftime("%Y%m%d")
            df = pro.index_daily(ts_code="000300.SH", start_date=start, end_date=end)
            if df is not None and not df.empty:
                df = df.sort_values("trade_date")
                closes = df["close"].astype(float).values
                if len(closes) >= 21:
                    returns = np.diff(np.log(closes[-21:]))
                    hv20 = float(np.std(returns) * np.sqrt(252) * 100)
                    result["æ²ªæ·±300_HV20"] = round(hv20, 1)
                    # æ³¢åŠ¨ç‡æ°´å¹³åˆ¤æ–­
                    if hv20 < 12:
                        result["æ³¢åŠ¨ç‡ç¯å¢ƒ"] = "ä½æ³¢åŠ¨"
                    elif hv20 < 20:
                        result["æ³¢åŠ¨ç‡ç¯å¢ƒ"] = "ä¸­ç­‰æ³¢åŠ¨"
                    elif hv20 < 30:
                        result["æ³¢åŠ¨ç‡ç¯å¢ƒ"] = "é«˜æ³¢åŠ¨"
                    else:
                        result["æ³¢åŠ¨ç‡ç¯å¢ƒ"] = "æç«¯æ³¢åŠ¨"

                # æˆäº¤é¢åŠ¨é‡: 5æ—¥å‡å€¼ vs 20æ—¥å‡å€¼
                amounts = df["amount"].astype(float).values
                if len(amounts) >= 20:
                    vol_5 = float(np.mean(amounts[-5:]))
                    vol_20 = float(np.mean(amounts[-20:]))
                    result["æˆäº¤é¢5/20æ¯”"] = round(vol_5 / vol_20, 2) if vol_20 > 0 else 1
                    result["é‡èƒ½çŠ¶æ€"] = "æ”¾é‡" if vol_5 / vol_20 > 1.2 else ("ç¼©é‡" if vol_5 / vol_20 < 0.8 else "æ¸©å’Œ")

            return result if result else None
        except Exception as e:
            logger.warning(f"[TS] æ³¢åŠ¨ç‡: {e}")
        return None

    result = _safe_call(_tushare_fetch, timeout=15, default=None, label="æ³¢åŠ¨ç‡[TS]")
    return result or {}


# ============================================================
# L10. æƒ…ç»ªæ¸©åº¦è®¡ (V4æ–°å¢ â€” ç»¼åˆå¤šç»´æŒ‡æ ‡)
# ============================================================
def get_sentiment_temperature(overview: dict = None, northbound: dict = None,
                               margin: dict = None, volatility: dict = None) -> dict:
    """
    æƒ…ç»ªç»¼åˆæ‰“åˆ† (0-100):
    - ä¸Šæ¶¨å æ¯” (æƒé‡25%)
    - åŒ—å‘èµ„é‡‘æ–¹å‘ (æƒé‡20%)
    - èèµ„ä½™é¢å˜åŒ– (æƒé‡20%)
    - æˆäº¤é¢åŠ¨é‡ (æƒé‡20%)
    - æ³¢åŠ¨ç‡é€†å‘ (æƒé‡15%, ä½æ³¢çœ‹å¤š)
    """
    score = 50  # ä¸­æ€§åŸºå‡†
    details = {}

    if overview:
        up_pct = overview.get("ä¸Šæ¶¨å æ¯”", 50)
        # ä¸Šæ¶¨å æ¯” > 60% ä¹è§‚, < 40% æ‚²è§‚
        s1 = min(max((up_pct - 30) / 40 * 100, 0), 100)
        details["èµšé’±æ•ˆåº”"] = round(s1, 0)
        score = score * 0.75 + s1 * 0.25

    if northbound:
        nb_val = northbound.get("ä»Šæ—¥å‡€æµå…¥äº¿", 0)
        s2 = min(max((nb_val + 100) / 200 * 100, 0), 100)
        details["åŒ—å‘æƒ…ç»ª"] = round(s2, 0)
        score = score * 0.80 + s2 * 0.20

    if margin:
        rz_chg = margin.get("èèµ„5æ—¥å˜åŒ–äº¿", 0)
        s3 = min(max((rz_chg + 200) / 400 * 100, 0), 100)
        details["æ æ†æƒ…ç»ª"] = round(s3, 0)
        score = score * 0.80 + s3 * 0.20

    if volatility:
        vol_ratio = volatility.get("æˆäº¤é¢5/20æ¯”", 1)
        s4 = min(max(vol_ratio * 50, 0), 100)
        details["é‡èƒ½æƒ…ç»ª"] = round(s4, 0)
        score = score * 0.80 + s4 * 0.20

        hv = volatility.get("æ²ªæ·±300_HV20", 15)
        s5 = min(max((30 - hv) / 20 * 100, 0), 100)  # ä½æ³¢ä¹è§‚
        details["æ³¢åŠ¨ç‡æƒ…ç»ª"] = round(s5, 0)
        score = score * 0.85 + s5 * 0.15

    temperature = round(score, 0)
    if temperature >= 70:
        level = "ğŸ”¥ è¿‡çƒ­ (è´ªå©ª)"
    elif temperature >= 55:
        level = "ğŸŸ¢ åæš– (ä¹è§‚)"
    elif temperature >= 45:
        level = "âšª ä¸­æ€§"
    elif temperature >= 30:
        level = "ğŸ”µ åå†· (è°¨æ…)"
    else:
        level = "â„ï¸ æå†· (ææƒ§)"

    return {
        "æ¸©åº¦": temperature,
        "çº§åˆ«": level,
        "åˆ†é¡¹": details,
    }


# ============================================================
# L11. æ¿å—æ•°æ® (AKShare ä¸ºä¸»)
# ============================================================
@st.cache_data(ttl=900, show_spinner=False)
def get_industry_board() -> pd.DataFrame:
    def _fetch():
        ak = _import_akshare()
        if not ak:
            return pd.DataFrame()
        df = ak.stock_board_industry_name_em()
        if df is not None and not df.empty:
            for c in ["æ¶¨è·Œå¹…", "æ€»å¸‚å€¼", "æ¢æ‰‹ç‡"]:
                if c in df.columns:
                    df[c] = pd.to_numeric(df[c], errors="coerce")
            return df.sort_values("æ¶¨è·Œå¹…", ascending=False).reset_index(drop=True)
        return pd.DataFrame()
    return _safe_call(_fetch, timeout=12, default=pd.DataFrame(), label="è¡Œä¸šæ¿å—")


@st.cache_data(ttl=900, show_spinner=False)
def get_concept_board() -> pd.DataFrame:
    def _fetch():
        ak = _import_akshare()
        if not ak:
            return pd.DataFrame()
        df = ak.stock_board_concept_name_em()
        if df is not None and not df.empty:
            for c in ["æ¶¨è·Œå¹…", "æ€»å¸‚å€¼", "æ¢æ‰‹ç‡"]:
                if c in df.columns:
                    df[c] = pd.to_numeric(df[c], errors="coerce")
            return df.sort_values("æ¶¨è·Œå¹…", ascending=False).reset_index(drop=True)
        return pd.DataFrame()
    return _safe_call(_fetch, timeout=12, default=pd.DataFrame(), label="æ¦‚å¿µæ¿å—")


# ============================================================
# L12. ETF â€” Tushare ä¼˜å…ˆ
# ============================================================
@st.cache_data(ttl=900, show_spinner=False)
def get_etf_list() -> pd.DataFrame:
    def _tushare_fetch():
        pro = _get_tushare_pro()
        if not pro:
            return None
        try:
            today = _last_trade_date()
            # è·å–ETFåŸºæœ¬ä¿¡æ¯
            df_basic = pro.fund_basic(market="E", status="L")
            if df_basic is None or df_basic.empty:
                return None
            # è·å–å½“æ—¥è¡Œæƒ… (å–å‰100åªä¸»è¦ETF)
            top_etfs = df_basic.head(150)
            rows = []
            for _, etf in top_etfs.iterrows():
                ts_code = etf.get("ts_code", "")
                name = etf.get("name", "")
                try:
                    df_q = pro.fund_daily(ts_code=ts_code, start_date=today, end_date=today)
                    if df_q is not None and not df_q.empty:
                        r = df_q.iloc[0]
                        rows.append({
                            "ä»£ç ": ts_code.split(".")[0],
                            "åç§°": name,
                            "æœ€æ–°ä»·": float(r.get("close", 0)),
                            "æ¶¨è·Œå¹…": float(r.get("pct_chg", 0)),
                            "æˆäº¤é¢": float(r.get("amount", 0)) * 1000,
                        })
                except Exception:
                    continue
                if len(rows) >= 80:
                    break
            if rows:
                df = pd.DataFrame(rows)
                df = df.sort_values("æˆäº¤é¢", ascending=False).reset_index(drop=True)
                return df
        except Exception as e:
            logger.warning(f"[TS] ETF: {e}")
        return None

    def _akshare_fetch():
        ak = _import_akshare()
        if not ak:
            return pd.DataFrame()
        df = ak.fund_etf_spot_em()
        if df is not None and not df.empty:
            for c in ["æœ€æ–°ä»·", "æ¶¨è·Œå¹…", "æˆäº¤é¢"]:
                if c in df.columns:
                    df[c] = pd.to_numeric(df[c], errors="coerce")
            return df.sort_values("æˆäº¤é¢", ascending=False).head(80).reset_index(drop=True)
        return pd.DataFrame()

    # ETFé€åªæŸ¥è¯¢å¤ªæ…¢, AKShareæ›´å¿«, ä¼˜å…ˆç”¨AKShare, Tushareä½œå¤‡é€‰
    result = _safe_call(_akshare_fetch, timeout=12, default=None, label="ETF[AK]")
    if result is not None and not result.empty:
        return result
    logger.info("[é™çº§] ETF â†’ Tushareé€åª")
    ts_result = _safe_call(_tushare_fetch, timeout=30, default=pd.DataFrame(), label="ETF[TS]")
    return ts_result if ts_result is not None else pd.DataFrame()


# ============================================================
# L13. èµ„è®¯é‡‡é›† â€” å¤šæºå¹¶è¡Œ (Tushare PRO)
# ============================================================
TUSHARE_NEWS_SOURCES = [
    ("cls",           "è´¢è”ç¤¾",     "T1", 150),
    ("yicai",         "ç¬¬ä¸€è´¢ç»",   "T1", 120),
    ("wallstreetcn",  "åå°”è¡—è§é—»", "T1", 120),
    ("eastmoney",     "ä¸œæ–¹è´¢å¯Œ",   "T2", 100),
    ("10jqka",        "åŒèŠ±é¡º",     "T2", 100),
    ("sina",          "æ–°æµªè´¢ç»",   "T2", 80),
    ("jinrongjie",    "é‡‘èç•Œ",     "T3", 50),
    ("yuncaijing",    "äº‘è´¢ç»",     "T3", 50),
]

_NOISE_WORDS = frozenset([
    "äº’åŠ¨å¹³å°", "äº’åŠ¨æ˜“", "æŠ½å¥–", "æŠ•èµ„è€…å…³ç³»", "åœç‰Œ", "å¤ç‰Œ",
    "æ–°è‚¡ç”³è´­", "å¤§å®—äº¤æ˜“", "è°ƒç ”ä¿¡æ¯", "äº¤æ˜“æç¤º", "ç›˜ä¸­å¼‚åŠ¨",
    "é¾™è™æ¦œ", "æˆäº¤å›æŠ¥", "æº¢ä»·ç‡", "ä¸­ç­¾å·", "é…å·",
])

_IMPORTANT_WORDS = frozenset([
    "å¤®è¡Œ", "å›½åŠ¡é™¢", "é™å‡†", "é™æ¯", "åŠ æ¯", "MLF", "LPR", "ç¤¾è",
    "GDP", "CPI", "PMI", "ä¸¤ä¼š", "æ”¿æ²»å±€", "è¯ç›‘ä¼š", "å‘æ”¹å§”",
    "ç¾è”å‚¨", "å…³ç¨", "åˆ¶è£", "æˆ˜äº‰", "åœ°éœ‡",
    "æš´è·Œ", "æš´æ¶¨", "ç†”æ–­", "æ¶¨åœæ½®", "è·Œåœæ½®", "åƒè‚¡",
])

_CATEGORY_RULES = {
    "å®è§‚æ”¿ç­–": ["å¤®è¡Œ", "å›½åŠ¡é™¢", "GDP", "CPI", "PPI", "PMI", "ç¤¾è", "M2",
                 "é™å‡†", "é™æ¯", "LPR", "MLF", "è´¢æ”¿", "ä¸¤ä¼š", "æ”¿æ²»å±€",
                 "å‘æ”¹å§”", "å·¥ä¿¡éƒ¨", "å•†åŠ¡éƒ¨", "è´¢æ”¿éƒ¨"],
    "æµ·å¤–å¸‚åœº": ["ç¾è”å‚¨", "ç¾å›½", "æ¬§æ´²", "æ—¥æœ¬", "ç¾è‚¡", "ç¾å€º", "ç¾å…ƒ",
                 "çº³æ–¯è¾¾å…‹", "é“ç¼æ–¯", "æ ‡æ™®", "å…³ç¨", "è‹±å›½", "æ—¥ç»"],
    "è¡Œä¸šäº§ä¸š": ["åŠå¯¼ä½“", "èŠ¯ç‰‡", "AI", "äººå·¥æ™ºèƒ½", "æœºå™¨äºº", "æ–°èƒ½æº", "å…‰ä¼",
                 "é”‚ç”µ", "å‚¨èƒ½", "åŒ»è¯", "åˆ›æ–°è¯", "å†›å·¥", "æ±½è½¦", "ç®—åŠ›",
                 "å¤§æ¨¡å‹", "ä½ç©ºç»æµ", "å•†ä¸šèˆªå¤©", "é‡å­"],
    "ç›‘ç®¡æ”¿ç­–": ["è¯ç›‘ä¼š", "é“¶ä¿ç›‘", "äº¤æ˜“æ‰€", "IPO", "æ³¨å†Œåˆ¶", "é€€å¸‚",
                 "å‡æŒ", "åˆ†çº¢", "å›è´­", "ç›‘ç®¡", "å¤„ç½š"],
    "å¸‚åœºèµ„é‡‘": ["åŒ—å‘", "å¤–èµ„", "èèµ„", "èåˆ¸", "ETF", "åŸºé‡‘", "ç¤¾ä¿",
                 "ä¿é™©èµ„é‡‘", "QFII", "ä¸»åŠ›", "æ¸¸èµ„"],
}


def _classify_news(title: str, content: str = "") -> tuple:
    text = title + content[:100]
    category = "ç»¼åˆè´¢ç»"
    for cat, keywords in _CATEGORY_RULES.items():
        if any(kw in text for kw in keywords):
            category = cat
            break
    is_important = any(w in text for w in _IMPORTANT_WORDS)
    return category, is_important


@st.cache_data(ttl=300, show_spinner=False)
def get_tushare_news(count: int = 150) -> list:
    """Tushare PRO å¤šæºå¹¶è¡Œé‡‡é›†å¼•æ“"""
    pro = _get_tushare_pro()
    if not pro:
        return []

    now = datetime.now()
    end_time = now.strftime("%Y-%m-%d %H:%M:%S")
    start_time = (now - timedelta(hours=24)).strftime("%Y-%m-%d %H:%M:%S")

    raw_news = []
    source_stats = {}

    for src, name, tier, limit in TUSHARE_NEWS_SOURCES:
        try:
            df = pro.news(src=src, start_date=start_time, end_date=end_time)
            fetched = 0
            if df is not None and not df.empty:
                for _, row in df.head(limit).iterrows():
                    title = str(row.get("title", "")).strip()
                    content = str(row.get("content", ""))[:600].strip()
                    dt = str(row.get("datetime", ""))
                    channels = str(row.get("channels", ""))
                    if not title or len(title) < 6:
                        continue
                    if any(w in title for w in _NOISE_WORDS):
                        continue
                    category, is_important = _classify_news(title, content)
                    pub_time = dt.split(" ")[1][:5] if " " in dt else dt[:16]
                    raw_news.append({
                        "time": pub_time, "datetime": dt, "title": title,
                        "content": content if content and content != title else title,
                        "important": is_important, "source": name, "source_id": src,
                        "tier": tier, "category": category, "channels": channels,
                    })
                    fetched += 1
            source_stats[name] = fetched
            logger.info(f"[é‡‡é›†] {name}({src}): {fetched} æ¡")
        except Exception as e:
            source_stats[name] = f"å¤±è´¥:{e}"
            logger.warning(f"[é‡‡é›†å¤±è´¥] {name}({src}): {e}")

    # æ–°é—»è”æ’­
    try:
        yesterday = (now - timedelta(days=1)).strftime("%Y%m%d")
        df_cctv = pro.cctv_news(date=yesterday)
        if df_cctv is not None and not df_cctv.empty:
            cctv_count = 0
            for _, row in df_cctv.head(15).iterrows():
                title = str(row.get("title", "")).strip()
                content = str(row.get("content", ""))[:400].strip()
                if title and len(title) > 5:
                    raw_news.append({
                        "time": "CCTV", "datetime": yesterday,
                        "title": f"[æ–°é—»è”æ’­] {title}", "content": content,
                        "important": True, "source": "æ–°é—»è”æ’­", "source_id": "cctv",
                        "tier": "T0", "category": "å®è§‚æ”¿ç­–", "channels": "",
                    })
                    cctv_count += 1
            source_stats["æ–°é—»è”æ’­"] = cctv_count
    except Exception as e:
        logger.warning(f"[é‡‡é›†å¤±è´¥] æ–°é—»è”æ’­: {e}")

    logger.info(f"[æ±‡æ€»] åŸå§‹ {len(raw_news)} æ¡ | {source_stats}")

    # æ™ºèƒ½å»é‡
    seen_titles = {}
    tier_priority = {"T0": 0, "T1": 1, "T2": 2, "T3": 3}
    for item in raw_news:
        key = item["title"][:30].strip()
        if key not in seen_titles:
            seen_titles[key] = item
        else:
            existing = seen_titles[key]
            if tier_priority.get(item["tier"], 9) < tier_priority.get(existing["tier"], 9):
                seen_titles[key] = item
    deduped = list(seen_titles.values())
    logger.info(f"[å»é‡] {len(raw_news)} â†’ {len(deduped)} æ¡")

    # è´¨é‡æ’åº (V4: å¢åŠ æ—¶é—´è¡°å‡)
    def _sort_key(item):
        tier_score = tier_priority.get(item["tier"], 9)
        important_score = 0 if item.get("important") else 1
        # æ—¶é—´è¡°å‡: è¶Šæ–°è¶Šé å‰
        dt_str = item.get("datetime", "")
        try:
            dt = datetime.strptime(dt_str[:19], "%Y-%m-%d %H:%M:%S")
            hours_ago = (now - dt).total_seconds() / 3600
            time_score = min(hours_ago / 24, 1)  # 0=åˆšå‘ 1=24hå‰
        except Exception:
            time_score = 0.5
        return (important_score, tier_score, time_score)

    deduped.sort(key=_sort_key)
    final = deduped[:count]
    logger.info(f"[è¾“å‡º] æœ€ç»ˆ {len(final)} æ¡ (ç›®æ ‡ {count})")
    return final


@st.cache_data(ttl=300, show_spinner=False)
def get_sina_flash(count: int = 30) -> list:
    """æ–°æµª 7Ã—24 å¿«è®¯ â€” é™çº§è¡¥å……"""
    telegraphs = []
    try:
        for page in range(1, 3):
            if len(telegraphs) >= count:
                break
            url = f"https://zhibo.sina.com.cn/api/zhibo/feed?page={page}&page_size=100&zhibo_id=152&tag_id=0&dire=f&dpc=1"
            headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/120.0.0.0"}
            resp = requests.get(url, headers=headers, timeout=6, verify=False)
            if resp.status_code == 200:
                items = resp.json().get("result", {}).get("data", {}).get("feed", {}).get("list", [])
                if not items:
                    break
                for item in items:
                    if len(telegraphs) >= count:
                        break
                    rich_text = item.get("rich_text", "")
                    if not rich_text:
                        continue
                    if "ã€‘" in rich_text and rich_text.startswith("ã€"):
                        parts = rich_text.split("ã€‘", 1)
                        title = parts[0].replace("ã€", "").strip()
                        content = parts[1].strip() if len(parts) > 1 else title
                    else:
                        title = rich_text[:60] + "..."
                        content = rich_text
                    if any(w in title for w in _NOISE_WORDS):
                        continue
                    time_str = item.get("create_time", "")
                    pub_time = time_str.split(" ")[1][:5] if " " in time_str else time_str
                    telegraphs.append({
                        "time": pub_time, "title": title, "content": content,
                        "important": False, "source": "æ–°æµªå¿«è®¯", "source_id": "sina_flash",
                        "tier": "T3", "category": "å¿«è®¯", "channels": "",
                    })
    except Exception as e:
        logger.warning(f"æ–°æµªå¿«è®¯æŠ“å–å¤±è´¥: {e}")
    return telegraphs[:count]


@st.cache_data(ttl=300, show_spinner=False)
def get_all_news(tushare_count: int = 150, sina_count: int = 0) -> list:
    """å…¨é‡èµ„è®¯èšåˆ â€” Tushare ä¸»åŠ›, æ–°æµªé™çº§å…œåº•"""
    all_news = []
    ts_news = get_tushare_news(tushare_count)
    all_news.extend(ts_news)

    if len(all_news) < 30:
        logger.warning(f"Tushare ä»… {len(all_news)} æ¡, å¯ç”¨æ–°æµªè¡¥å……")
        sina_news = get_sina_flash(max(sina_count, 50))
        existing = set(n["title"][:20] for n in all_news)
        for item in sina_news:
            if item["title"][:20] not in existing:
                all_news.append(item)
                existing.add(item["title"][:20])

    src_counts = {}
    for n in all_news:
        src = n.get("source", "unknown")
        src_counts[src] = src_counts.get(src, 0) + 1
    logger.info(f"[èšåˆ] å…± {len(all_news)} æ¡ | {src_counts}")
    return all_news


# ============================================================
# L14. åˆ¸å•†ç ”æŠ¥ (Tushare PRO)
# ============================================================
@st.cache_data(ttl=3600, show_spinner=False)
def get_research_reports(count: int = 30) -> list:
    pro = _get_tushare_pro()
    if not pro:
        return []
    reports = []
    try:
        today = datetime.now().strftime("%Y%m%d")
        start = (datetime.now() - timedelta(days=5)).strftime("%Y%m%d")
        df = pro.report_rc(start_date=start, end_date=today)
        if df is not None and not df.empty:
            if "report_date" in df.columns:
                df = df.sort_values("report_date", ascending=False)
            for _, row in df.head(count).iterrows():
                reports.append({
                    "stock_name": str(row.get("name", row.get("ts_code", ""))),
                    "ts_code": str(row.get("ts_code", "")),
                    "org_name": str(row.get("org_name", "")),
                    "rating": str(row.get("rating", "")),
                    "pre_rating": str(row.get("pre_rating", "")),
                    "target_price": row.get("target_price", None),
                    "report_date": str(row.get("report_date", "")),
                    "title": str(row.get("title", "")),
                })
            logger.info(f"åˆ¸å•†ç ”æŠ¥: {len(reports)} æ¡")
    except Exception as e:
        logger.warning(f"åˆ¸å•†ç ”æŠ¥å¤±è´¥: {e}")
    return reports


# ============================================================
# L15. å•†å“æœŸè´§ (AKShare)
# ============================================================
@st.cache_data(ttl=900, show_spinner=False)
def get_futures_overview() -> dict:
    def _fetch():
        ak = _import_akshare()
        if not ak:
            return {}
        result = {}
        try:
            df = ak.futures_main_sina()
            if df is not None and not df.empty:
                key_items = {
                    "æ²ªé‡‘": "é»„é‡‘", "æ²ªé“¶": "ç™½é“¶", "æ²ªé“œ": "é“œ",
                    "èºçº¹": "èºçº¹é’¢", "é“çŸ¿": "é“çŸ¿çŸ³",
                    "åŸæ²¹": "åŸæ²¹", "æ²ªé“": "é“",
                    "è±†ç²•": "è±†ç²•", "æ£•æ¦ˆ": "æ£•æ¦ˆæ²¹",
                }
                name_col = None
                for col in df.columns:
                    if "å" in str(col) or "å“ç§" in str(col) or "symbol" in str(col).lower():
                        name_col = col
                        break
                if name_col is None and len(df.columns) > 0:
                    name_col = df.columns[0]
                for _, row in df.iterrows():
                    name = str(row.get(name_col, "")) if name_col else ""
                    for key, display in key_items.items():
                        if key in name:
                            chg_col = [c for c in df.columns if "æ¶¨è·Œ" in str(c) and "å¹…" in str(c)]
                            price_col = [c for c in df.columns if "æœ€æ–°" in str(c) or "æ”¶" in str(c)]
                            chg = float(row[chg_col[0]]) if chg_col else 0
                            price = str(row[price_col[0]]) if price_col else "â€”"
                            result[display] = {"price": price, "chg_pct": round(chg, 2)}
                            break
        except Exception as e:
            logger.warning(f"æœŸè´§è¡Œæƒ…å¤±è´¥: {e}")
        return result
    return _safe_call(_fetch, timeout=10, default={}, label="æœŸè´§è¡Œæƒ…")


# ============================================================
# L16. å…¨é‡æ•°æ®æ‰“åŒ… (ä¾› AI CIOæ—¥æŠ¥)
# ============================================================
def get_daily_data_pack() -> dict:
    """ä¸€æ¬¡æ€§è·å–æ‰€æœ‰æ•°æ®"""
    return {
        "indices": get_major_indices(),
        "overview": get_market_overview(),
        "industry": get_industry_board(),
        "concept": get_concept_board(),
        "macro": get_macro_data(),
        "liquidity": get_liquidity_data(),
        "credit": get_credit_spread(),
        "style": get_style_data(),
        "volatility": get_volatility_data(),
        "etf": get_etf_list(),
        "northbound": get_northbound_flow(),
        "margin": get_margin_data(),
        "futures": get_futures_overview(),
        "news": get_all_news(tushare_count=150),
        "research": get_research_reports(30),
        "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M"),
    }


def pack_market_text(pack: dict) -> str:
    """å°†æ•°æ®åŒ…è½¬ä¸ºæ–‡æœ¬ (ä¾›AI prompt) â€” V4: æ›´å…¨é¢ä¸¥è°¨"""
    mp = ["## ä»Šæ—¥å¸‚åœºæ•°æ® (æ•°æ®æˆªè‡³ {})".format(pack.get("timestamp", ""))]

    # æŒ‡æ•°è¡Œæƒ…
    idx = pack.get("indices")
    if idx is not None and not idx.empty:
        mp.append("\n### å®½åŸºæŒ‡æ•°")
        for _, r in idx.iterrows():
            chg = r.get("æ¶¨è·Œå¹…", 0)
            if pd.notna(chg):
                mp.append(f"- {r.get('åç§°','')}: {r.get('æœ€æ–°ä»·','')} ({chg:+.2f}%)")

    # æ¶¨è·Œç»Ÿè®¡
    ov = pack.get("overview", {})
    if ov:
        mp.append(f"\n### å¸‚åœºæƒ…ç»ª")
        mp.append(f"æ¶¨{ov.get('ä¸Šæ¶¨',0)} è·Œ{ov.get('ä¸‹è·Œ',0)} | æ¶¨åœ{ov.get('æ¶¨åœ',0)} è·Œåœ{ov.get('è·Œåœ',0)} | æˆäº¤{ov.get('æ€»æˆäº¤é¢äº¿',0):.0f}äº¿ | ä¸Šæ¶¨å æ¯”{ov.get('ä¸Šæ¶¨å æ¯”',0)}%")
        mp.append(f"å¼ºåŠ¿è‚¡(>3%): {ov.get('å¼ºåŠ¿è‚¡',0)} | å¼±åŠ¿è‚¡(<-3%): {ov.get('å¼±åŠ¿è‚¡',0)}")

    # å®è§‚æ•°æ® (æ¡¥æ°´å››ç»´)
    macro = pack.get("macro", {})
    if macro:
        items = [f"{k}:{v}" for k, v in macro.items() if v not in ("", None) and "æœˆä»½" not in k]
        if items:
            mp.append(f"\n### å®è§‚ç»æµ (å¢é•¿+é€šèƒ€)")
            mp.append(" | ".join(items))

    # æµåŠ¨æ€§
    liq = pack.get("liquidity", {})
    if liq:
        items = [f"{k}:{v}" for k, v in liq.items() if v not in ("", None)]
        if items:
            mp.append(f"\n### æµåŠ¨æ€§")
            mp.append(" | ".join(items))

    # ä¿¡ç”¨
    credit = pack.get("credit", {})
    if credit:
        items = [f"{k}:{v}" for k, v in credit.items() if v not in ("", None)]
        if items:
            mp.append(f"\n### ä¿¡ç”¨ç¯å¢ƒ")
            mp.append(" | ".join(items))

    # é£æ ¼ (5æ—¥+20æ—¥)
    style = pack.get("style", {})
    if style:
        mp.append(f"\n### å¸‚åœºé£æ ¼")
        s_parts = []
        if "å¤§å°ç›˜åå¥½_5æ—¥" in style:
            s_parts.append(f"å¤§å°ç›˜(5æ—¥):{style['å¤§å°ç›˜åå¥½_5æ—¥']}(300:{style.get('æ²ªæ·±300_5æ—¥','')}%/1000:{style.get('ä¸­è¯1000_5æ—¥','')}%)")
        if "å¤§å°ç›˜åå¥½_20æ—¥" in style:
            s_parts.append(f"å¤§å°ç›˜(20æ—¥):{style['å¤§å°ç›˜åå¥½_20æ—¥']}(300:{style.get('æ²ªæ·±300_20æ—¥','')}%/1000:{style.get('ä¸­è¯1000_20æ—¥','')}%)")
        if "æˆé•¿ä»·å€¼_5æ—¥" in style:
            s_parts.append(f"æˆé•¿ä»·å€¼(5æ—¥):{style['æˆé•¿ä»·å€¼_5æ—¥']}(åˆ›ä¸šæ¿:{style.get('åˆ›ä¸šæ¿æŒ‡_5æ—¥','')}%/50:{style.get('ä¸Šè¯50_5æ—¥','')}%)")
        if "æˆé•¿ä»·å€¼_20æ—¥" in style:
            s_parts.append(f"æˆé•¿ä»·å€¼(20æ—¥):{style['æˆé•¿ä»·å€¼_20æ—¥']}(åˆ›ä¸šæ¿:{style.get('åˆ›ä¸šæ¿æŒ‡_20æ—¥','')}%/50:{style.get('ä¸Šè¯50_20æ—¥','')}%)")
        if s_parts:
            mp.append(" | ".join(s_parts))

    # æ³¢åŠ¨ç‡
    vol = pack.get("volatility", {})
    if vol:
        mp.append(f"\n### æ³¢åŠ¨ç‡ä¸é‡èƒ½")
        v_parts = []
        if "æ²ªæ·±300_HV20" in vol:
            v_parts.append(f"æ²ªæ·±300 HV20:{vol['æ²ªæ·±300_HV20']}% ({vol.get('æ³¢åŠ¨ç‡ç¯å¢ƒ','')})")
        if "æˆäº¤é¢5/20æ¯”" in vol:
            v_parts.append(f"æˆäº¤é¢5/20æ—¥æ¯”:{vol['æˆäº¤é¢5/20æ¯”']} ({vol.get('é‡èƒ½çŠ¶æ€','')})")
        if v_parts:
            mp.append(" | ".join(v_parts))

    # åŒ—å‘èµ„é‡‘
    nb = pack.get("northbound", {})
    if nb:
        mp.append(f"\n### èµ„é‡‘æµå‘")
        mp.append(f"åŒ—å‘: ä»Šæ—¥{nb.get('æ–¹å‘','')}{nb.get('ä»Šæ—¥å‡€æµå…¥äº¿',0)}äº¿ | 5æ—¥å‡å€¼{nb.get('5æ—¥å‡å€¼äº¿',0)}äº¿")

    margin = pack.get("margin", {})
    if margin:
        mp.append(f"èèµ„: ä½™é¢{margin.get('èèµ„ä½™é¢äº¿',0)}äº¿ | 5æ—¥å˜åŒ–{margin.get('èèµ„5æ—¥å˜åŒ–äº¿',0)}äº¿ | {margin.get('æ æ†æƒ…ç»ª','')}")

    # æœŸè´§
    futures = pack.get("futures", {})
    if futures:
        f_parts = []
        for k, v in futures.items():
            chg = v.get("chg_pct", 0)
            f_parts.append(f"{k}:{v.get('price','')}" + (f"({chg:+.1f}%)" if chg else ""))
        if f_parts:
            mp.append(f"\n### å•†å“æœŸè´§")
            mp.append(" | ".join(f_parts))

    # è¡Œä¸š
    ind = pack.get("industry")
    if ind is not None and not ind.empty and "æ¿å—åç§°" in ind.columns and "æ¶¨è·Œå¹…" in ind.columns:
        mp.append(f"\n### è¡Œä¸šæ¿å—")
        mp.append("æ¶¨å¹…TOP5: " + ", ".join(f"{r['æ¿å—åç§°']}({r['æ¶¨è·Œå¹…']:+.1f}%)" for _, r in ind.head(5).iterrows()))
        mp.append("è·Œå¹…TOP5: " + ", ".join(f"{r['æ¿å—åç§°']}({r['æ¶¨è·Œå¹…']:+.1f}%)" for _, r in ind.tail(5).iterrows()))

    # æƒ…ç»ªæ¸©åº¦è®¡
    sentiment = get_sentiment_temperature(ov, nb, margin, vol)
    if sentiment:
        mp.append(f"\n### æƒ…ç»ªæ¸©åº¦è®¡")
        mp.append(f"ç»¼åˆæ¸©åº¦: {sentiment['æ¸©åº¦']} ({sentiment['çº§åˆ«']})")
        for k, v in sentiment.get("åˆ†é¡¹", {}).items():
            mp.append(f"  - {k}: {v}")

    return "\n".join(mp)


def pack_news_text(pack: dict) -> str:
    """å°†èµ„è®¯+ç ”æŠ¥è½¬ä¸ºæ–‡æœ¬ â€” V4ç‰ˆ"""
    np_list = []
    news = pack.get("news", [])

    src_counts = {}
    for n in news:
        src = n.get("source", "unknown")
        src_counts[src] = src_counts.get(src, 0) + 1
    src_summary = ", ".join(f"{k}:{v}" for k, v in src_counts.items())
    np_list.append(f"## ä»Šæ—¥èµ„è®¯ (å…±{len(news)}æ¡, æ¥æº: {src_summary})")

    important = [n for n in news if n.get("important")]
    t1_news = [n for n in news if not n.get("important") and n.get("tier") in ("T0", "T1")]
    t2_news = [n for n in news if not n.get("important") and n.get("tier") in ("T2", "T3", None)]

    if important:
        np_list.append("\n### â­ é‡è¦èµ„è®¯")
        for n in important[:20]:
            np_list.append(f"â­[{n.get('source','')}][{n.get('category','')}] {n.get('title','')}")
            if n.get("content") and n["content"] != n.get("title"):
                np_list.append(f"   æ‘˜è¦: {n['content'][:250]}")

    if t1_news:
        np_list.append("\n### æœºæ„çº§èµ„è®¯")
        for n in t1_news[:50]:
            line = f"- [{n.get('source','')}][{n.get('category','')}] {n.get('title','')}"
            content = n.get("content", "")
            if content and content != n.get("title") and len(content) > 20:
                line += f" | {content[:120]}"
            np_list.append(line)

    if t2_news:
        np_list.append("\n### ç»¼åˆèµ„è®¯")
        for n in t2_news[:40]:
            np_list.append(f"- [{n.get('source','')}] {n.get('title','')}")

    research = pack.get("research", [])
    if research:
        np_list.append(f"\n## åˆ¸å•†ç ”æŠ¥åŠ¨æ€ ({len(research)}æ¡)")
        for r in research[:25]:
            rating_chg = ""
            if r.get("pre_rating") and r.get("rating") and r["pre_rating"] != r["rating"]:
                rating_chg = f" (ä»{r['pre_rating']}â†’{r['rating']})"
            elif r.get("rating"):
                rating_chg = f" ({r['rating']})"
            target = f" ç›®æ ‡ä»·{r['target_price']}" if r.get("target_price") else ""
            np_list.append(f"- {r.get('org_name','')}: {r.get('stock_name','')}{rating_chg}{target}")

    return "\n".join(np_list)


# ============================================================
# Q1. è¡Œä¸šèµ„é‡‘æµå‘ (Tushare PRO)
# ============================================================
@st.cache_data(ttl=600, show_spinner=False)
def get_industry_moneyflow() -> pd.DataFrame:
    """è¡Œä¸šèµ„é‡‘æµå‘ â€” è¯†åˆ«èµ„é‡‘ä¸»æ”»æ–¹å‘"""
    def _tushare_fetch():
        pro = _get_tushare_pro()
        if not pro:
            return None
        try:
            today = _last_trade_date()
            df = pro.moneyflow_ind_dc(trade_date=today)
            if df is None or df.empty:
                for offset in range(1, 4):
                    df = pro.moneyflow_ind_dc(trade_date=_last_trade_date(offset))
                    if df is not None and not df.empty:
                        break
            if df is not None and not df.empty:
                for c in ["net_amount", "buy_elg_amount", "buy_lg_amount",
                           "buy_md_amount", "buy_sm_amount"]:
                    if c in df.columns:
                        df[c] = pd.to_numeric(df[c], errors="coerce")
                df = df.sort_values("net_amount", ascending=False).reset_index(drop=True)
                return df
        except Exception as e:
            logger.warning(f"[TS] è¡Œä¸šèµ„é‡‘æµå‘: {e}")
        return None

    return _safe_call(_tushare_fetch, timeout=15, default=pd.DataFrame(), label="è¡Œä¸šèµ„é‡‘æµ[TS]")


# ============================================================
# Q2. ä¸ªè‚¡æ—¥çº¿è¡Œæƒ… (Tushare PRO â€” é‡åŒ–é€‰è‚¡æ ¸å¿ƒ)
# ============================================================
@st.cache_data(ttl=300, show_spinner=False)
def get_stock_daily(ts_code: str, days: int = 120) -> pd.DataFrame:
    """è·å–å•åªè‚¡ç¥¨æ—¥çº¿è¡Œæƒ… (V4.1 æ¶æ„å¸ˆä¿®æ­£ï¼šå¼ºåˆ¶å‰å¤æƒ)"""
    pro = _get_tushare_pro()
    if not pro:
        return pd.DataFrame()
    try:
        import tushare as ts
        end = _last_trade_date()
        start = (datetime.now() - timedelta(days=days * 1.5)).strftime("%Y%m%d")
        
        # æç®€ä»¤ç‰Œæ¡¶é™æµï¼Œä¿æŠ¤ API Token
        time.sleep(0.05) 
        # æ ¸å¿ƒä¿®æ­£ï¼šå¼ºåˆ¶ä½¿ç”¨ pro_bar å¹¶åœ¨æºå¤´æ‰§è¡Œ qfq
        df = ts.pro_bar(ts_code=ts_code, api=pro, adj='qfq', start_date=start, end_date=end)
        
        if df is not None and not df.empty:
            df = df.sort_values("trade_date").reset_index(drop=True)
            for c in ["open", "high", "low", "close", "vol", "amount", "pct_chg"]:
                if c in df.columns:
                    df[c] = pd.to_numeric(df[c], errors="coerce")
            return df
    except Exception as e:
        logger.warning(f"[TS] ä¸ªè‚¡æ—¥çº¿ {ts_code}: {e}")
    return pd.DataFrame()


# ============================================================
# Q3. ä¸ªè‚¡èµ„é‡‘æµå‘ (Tushare PRO)
# ============================================================
@st.cache_data(ttl=600, show_spinner=False)
def get_stock_moneyflow(ts_code: str, days: int = 20) -> pd.DataFrame:
    """ä¸ªè‚¡èµ„é‡‘æµå‘ â€” ä¸»åŠ›/è¶…å¤§å•/å¤§å•"""
    pro = _get_tushare_pro()
    if not pro:
        return pd.DataFrame()
    try:
        end = _last_trade_date()
        start = (datetime.now() - timedelta(days=days * 2)).strftime("%Y%m%d")
        df = pro.moneyflow(ts_code=ts_code, start_date=start, end_date=end)
        if df is not None and not df.empty:
            df = df.sort_values("trade_date").reset_index(drop=True)
            for c in df.columns:
                if c != "trade_date" and c != "ts_code":
                    df[c] = pd.to_numeric(df[c], errors="coerce")
            return df
    except Exception as e:
        logger.warning(f"[TS] ä¸ªè‚¡èµ„é‡‘æµ {ts_code}: {e}")
    return pd.DataFrame()


# ============================================================
# Q4. å…¨å¸‚åœºè¡Œæƒ…å¿«ç…§ (Tushare daily â€” ç”¨äºæ‰¹é‡ç­›é€‰)
# ============================================================
@st.cache_data(ttl=600, show_spinner=False)
def get_market_snapshot() -> pd.DataFrame:
    """å…¨Aè‚¡ä»Šæ—¥è¡Œæƒ…å¿«ç…§ â€” é‡åŒ–é€‰è‚¡çš„åŸºç¡€æ•°æ®"""
    def _tushare_fetch():
        pro = _get_tushare_pro()
        if not pro:
            return None
        try:
            today = _last_trade_date()
            df = pro.daily(trade_date=today)
            if df is None or df.empty:
                for offset in range(1, 4):
                    df = pro.daily(trade_date=_last_trade_date(offset))
                    if df is not None and not df.empty:
                        break
            if df is not None and not df.empty:
                for c in ["open", "high", "low", "close", "pre_close",
                           "change", "pct_chg", "vol", "amount"]:
                    if c in df.columns:
                        df[c] = pd.to_numeric(df[c], errors="coerce")
                # åŸºç¡€åç§°æ˜ å°„
                try:
                    basic = pro.stock_basic(exchange='', list_status='L',
                                             fields='ts_code,symbol,name,industry,area')
                    if basic is not None and not basic.empty:
                        df = df.merge(basic[["ts_code", "name", "industry"]], on="ts_code", how="left")
                except Exception:
                    pass
                return df
        except Exception as e:
            logger.warning(f"[TS] å…¨å¸‚åœºå¿«ç…§: {e}")
        return None

    def _akshare_fetch():
        ak = _import_akshare()
        if not ak:
            return pd.DataFrame()
        try:
            df = ak.stock_zh_a_spot_em()
            if df is not None and not df.empty:
                for c in ["æœ€æ–°ä»·", "æ¶¨è·Œå¹…", "æ¶¨è·Œé¢", "æˆäº¤é‡", "æˆäº¤é¢",
                           "æŒ¯å¹…", "æ¢æ‰‹ç‡", "é‡æ¯”"]:
                    if c in df.columns:
                        df[c] = pd.to_numeric(df[c], errors="coerce")
                return df
        except Exception:
            pass
        return pd.DataFrame()

    result = _safe_call(_tushare_fetch, timeout=20, default=None, label="å¸‚åœºå¿«ç…§[TS]")
    if result is not None and not result.empty:
        return result
    logger.info("[é™çº§] å¸‚åœºå¿«ç…§ â†’ AKShare")
    return _safe_call(_akshare_fetch, timeout=15, default=pd.DataFrame(), label="å¸‚åœºå¿«ç…§[AK]")


# ============================================================
# Q5. å†å²æ—¥çº¿æ‰¹é‡ (Tushare â€” ç”¨äºè®¡ç®—æŠ€æœ¯æŒ‡æ ‡)
# ============================================================
@st.cache_data(ttl=600, show_spinner=False)
def get_multi_stock_daily(ts_codes: list, days: int = 60) -> dict:
    """æ‰¹é‡è·å–å¤šåªè‚¡ç¥¨æ—¥çº¿ â€” ç”¨äºå› å­è®¡ç®— (V4.1 æ¶æ„å¸ˆä¿®æ­£ï¼šå¼ºåˆ¶å‰å¤æƒ)"""
    pro = _get_tushare_pro()
    if not pro:
        return {}
        
    import tushare as ts
    result = {}
    end = _last_trade_date()
    start = (datetime.now() - timedelta(days=days * 1.5)).strftime("%Y%m%d")
    
    for ts_code in ts_codes[:50]:  # æœ€å¤š50åªï¼Œé¿å…APIè¿‡è½½
        try:
            # æç®€ä»¤ç‰Œæ¡¶é™æµï¼Œä¿æŠ¤ API Token
            time.sleep(0.05) 
            # æ ¸å¿ƒä¿®æ­£ï¼šå¼ºåˆ¶ä½¿ç”¨ pro_bar å¹¶åœ¨æºå¤´æ‰§è¡Œ qfq
            df = ts.pro_bar(ts_code=ts_code, api=pro, adj='qfq', start_date=start, end_date=end)
            
            if df is not None and not df.empty:
                df = df.sort_values("trade_date").reset_index(drop=True)
                for c in ["open", "high", "low", "close", "vol", "amount", "pct_chg"]:
                    if c in df.columns:
                        df[c] = pd.to_numeric(df[c], errors="coerce")
                result[ts_code] = df
        except Exception as e:
            logger.warning(f"[TS] æ—¥çº¿ {ts_code}: {e}")
    return result


# ============================================================
# Q6. ä¸ªè‚¡èµ„é‡‘æµå‘æ‰¹é‡ (Tushare â€” æŒ‰æ—¥æœŸè·å–å…¨å¸‚åœº)
# ============================================================
@st.cache_data(ttl=600, show_spinner=False)
def get_market_moneyflow(date: str = None) -> pd.DataFrame:
    """å…¨å¸‚åœºä¸ªè‚¡èµ„é‡‘æµå‘ â€” æŒ‰æ—¥æœŸ"""
    pro = _get_tushare_pro()
    if not pro:
        return pd.DataFrame()
    try:
        if not date:
            date = _last_trade_date()
        df = pro.moneyflow(trade_date=date)
        if df is None or df.empty:
            for offset in range(1, 4):
                df = pro.moneyflow(trade_date=_last_trade_date(offset))
                if df is not None and not df.empty:
                    break
        if df is not None and not df.empty:
            for c in df.columns:
                if c not in ("trade_date", "ts_code"):
                    df[c] = pd.to_numeric(df[c], errors="coerce")
            return df
    except Exception as e:
        logger.warning(f"[TS] å…¨å¸‚åœºèµ„é‡‘æµ: {e}")
    return pd.DataFrame()


# ============================================================
# Q7. é‡åŒ–å› å­è®¡ç®—å¼•æ“
# ============================================================
def calc_technical_factors(df: pd.DataFrame) -> dict:
    """
    åŸºäºæ—¥çº¿æ•°æ®è®¡ç®—æŠ€æœ¯å› å­
    è¾“å…¥: å•åªè‚¡ç¥¨çš„æ—¥çº¿ DataFrame (éœ€å« close/high/low/vol/amount/pct_chg)
    è¾“å‡º: å› å­å­—å…¸
    """
    if df is None or df.empty or len(df) < 20:
        return {}

    close = df["close"].values.astype(float)
    high = df["high"].values.astype(float)
    low = df["low"].values.astype(float)
    vol = df["vol"].values.astype(float)
    pct_chg = df["pct_chg"].values.astype(float) if "pct_chg" in df.columns else np.zeros(len(close))

    factors = {}

    # === è¶‹åŠ¿åŠ¨é‡ ===
    if len(close) >= 5:
        factors["åŠ¨é‡_5æ—¥"] = round((close[-1] / close[-5] - 1) * 100, 2)
    if len(close) >= 10:
        factors["åŠ¨é‡_10æ—¥"] = round((close[-1] / close[-10] - 1) * 100, 2)
    if len(close) >= 20:
        factors["åŠ¨é‡_20æ—¥"] = round((close[-1] / close[-20] - 1) * 100, 2)
    if len(close) >= 60:
        factors["åŠ¨é‡_60æ—¥"] = round((close[-1] / close[-60] - 1) * 100, 2)

    # === å‡çº¿ç³»ç»Ÿ ===
    if len(close) >= 5:
        ma5 = np.mean(close[-5:])
        factors["MA5"] = round(ma5, 2)
        factors["ä»·æ ¼/MA5"] = round(close[-1] / ma5, 4)
    if len(close) >= 10:
        ma10 = np.mean(close[-10:])
        factors["MA10"] = round(ma10, 2)
    if len(close) >= 20:
        ma20 = np.mean(close[-20:])
        factors["MA20"] = round(ma20, 2)
        factors["ä»·æ ¼/MA20"] = round(close[-1] / ma20, 4)
    if len(close) >= 60:
        ma60 = np.mean(close[-60:])
        factors["MA60"] = round(ma60, 2)
        factors["ä»·æ ¼/MA60"] = round(close[-1] / ma60, 4)

    # MA5 > MA20 é‡‘å‰
    if "MA5" in factors and "MA20" in factors:
        factors["å‡çº¿å¤šå¤´"] = 1 if factors["MA5"] > factors["MA20"] else 0

    # === MACD ===
    if len(close) >= 35:
        ema12 = _ema(close, 12)
        ema26 = _ema(close, 26)
        dif = ema12 - ema26
        dea = _ema(dif, 9)
        macd_bar = (dif - dea) * 2
        factors["MACD_DIF"] = round(dif[-1], 3)
        factors["MACD_DEA"] = round(dea[-1], 3)
        factors["MACDæŸ±"] = round(macd_bar[-1], 3)
        factors["MACDé‡‘å‰"] = 1 if dif[-1] > dea[-1] and dif[-2] <= dea[-2] else 0

    # === RSI ===
    if len(pct_chg) >= 14:
        gains = np.where(pct_chg[-14:] > 0, pct_chg[-14:], 0)
        losses = np.where(pct_chg[-14:] < 0, -pct_chg[-14:], 0)
        avg_gain = np.mean(gains)
        avg_loss = np.mean(losses)
        if avg_loss > 0:
            rs = avg_gain / avg_loss
            factors["RSI_14"] = round(100 - 100 / (1 + rs), 1)
        else:
            factors["RSI_14"] = 100.0

    # === å¸ƒæ—å¸¦ ===
    if len(close) >= 20:
        ma20 = np.mean(close[-20:])
        std20 = np.std(close[-20:])
        upper = ma20 + 2 * std20
        lower = ma20 - 2 * std20
        factors["å¸ƒæ—ä¸Šè½¨"] = round(upper, 2)
        factors["å¸ƒæ—ä¸‹è½¨"] = round(lower, 2)
        if upper != lower:
            factors["å¸ƒæ—ä½ç½®"] = round((close[-1] - lower) / (upper - lower), 2)  # 0=ä¸‹è½¨ 1=ä¸Šè½¨

    # === ATR (æ³¢åŠ¨ç‡) ===
    if len(close) >= 15:
        tr = np.maximum(high[-14:] - low[-14:],
                        np.maximum(abs(high[-14:] - close[-15:-1]),
                                   abs(low[-14:] - close[-15:-1])))
        atr14 = np.mean(tr)
        factors["ATR_14"] = round(atr14, 2)
        factors["ATR/ä»·æ ¼%"] = round(atr14 / close[-1] * 100, 2)

    # === é‡ä»·å…³ç³» ===
    if len(vol) >= 20:
        vol5 = np.mean(vol[-5:])
        vol20 = np.mean(vol[-20:])
        factors["é‡æ¯”_5/20"] = round(vol5 / vol20, 2) if vol20 > 0 else 1
        factors["ä»Šæ—¥é‡æ¯”"] = round(vol[-1] / vol20, 2) if vol20 > 0 else 1
    if len(vol) >= 5:
        factors["5æ—¥å‡é‡"] = round(np.mean(vol[-5:]), 0)

    # === è¿‘Næ—¥æ–°é«˜/æ–°ä½ ===
    if len(high) >= 20:
        factors["20æ—¥æ–°é«˜"] = 1 if close[-1] >= np.max(high[-20:]) * 0.98 else 0
    if len(high) >= 60:
        factors["60æ—¥æ–°é«˜"] = 1 if close[-1] >= np.max(high[-60:]) * 0.98 else 0
    if len(low) >= 20:
        factors["20æ—¥æ–°ä½"] = 1 if close[-1] <= np.min(low[-20:]) * 1.02 else 0

    # === è¿æ¶¨/è¿è·Œå¤©æ•° ===
    if len(pct_chg) >= 2:
        consecutive = 0
        for i in range(len(pct_chg) - 1, -1, -1):
            if pct_chg[i] > 0:
                consecutive += 1
            else:
                break
        factors["è¿æ¶¨å¤©æ•°"] = consecutive

    return factors


def _ema(data, period):
    """æŒ‡æ•°ç§»åŠ¨å¹³å‡"""
    if len(data) < period:
        return data
    alpha = 2 / (period + 1)
    result = np.zeros_like(data, dtype=float)
    result[0] = data[0]
    for i in range(1, len(data)):
        result[i] = alpha * data[i] + (1 - alpha) * result[i - 1]
    return result


def calc_moneyflow_factors(mf_df: pd.DataFrame) -> dict:
    """åŸºäºèµ„é‡‘æµå‘æ•°æ®è®¡ç®—å› å­"""
    if mf_df is None or mf_df.empty:
        return {}
    factors = {}
    try:
        # æœ€è¿‘ä¸€æ—¥
        latest = mf_df.iloc[-1]
        # ä¸»åŠ›å‡€æµå…¥ (è¶…å¤§å•+å¤§å•)
        buy_elg = float(latest.get("buy_elg_vol", 0))
        sell_elg = float(latest.get("sell_elg_vol", 0))
        buy_lg = float(latest.get("buy_lg_vol", 0))
        sell_lg = float(latest.get("sell_lg_vol", 0))
        net_main = (buy_elg - sell_elg + buy_lg - sell_lg)
        factors["ä¸»åŠ›å‡€æµå…¥"] = round(net_main, 0)

        # 5æ—¥ç´¯è®¡ä¸»åŠ›å‡€æµå…¥
        if len(mf_df) >= 5:
            recent5 = mf_df.tail(5)
            net5 = 0
            for _, r in recent5.iterrows():
                net5 += (float(r.get("buy_elg_vol", 0)) - float(r.get("sell_elg_vol", 0))
                         + float(r.get("buy_lg_vol", 0)) - float(r.get("sell_lg_vol", 0)))
            factors["ä¸»åŠ›å‡€æµå…¥_5æ—¥"] = round(net5, 0)

        # è¶…å¤§å•å æ¯”
        total_vol = float(latest.get("buy_elg_vol", 0)) + float(latest.get("sell_elg_vol", 0))
        total_all = float(latest.get("trade_count", 1)) if latest.get("trade_count") else 1
        if total_all > 0:
            factors["è¶…å¤§å•å æ¯”"] = round(total_vol / max(total_all, 1) * 100, 1)

        # è¿ç»­å‡€æµå…¥å¤©æ•°
        consecutive = 0
        for i in range(len(mf_df) - 1, -1, -1):
            r = mf_df.iloc[i]
            net = (float(r.get("buy_elg_vol", 0)) - float(r.get("sell_elg_vol", 0))
                   + float(r.get("buy_lg_vol", 0)) - float(r.get("sell_lg_vol", 0)))
            if net > 0:
                consecutive += 1
            else:
                break
        factors["ä¸»åŠ›è¿ç»­æµå…¥å¤©æ•°"] = consecutive

    except Exception as e:
        logger.warning(f"èµ„é‡‘å› å­è®¡ç®—å¼‚å¸¸: {e}")
    return factors


# ============================================================
# Q8. å¤šå› å­ç»¼åˆé€‰è‚¡å¼•æ“
# ============================================================
def quant_stock_screener(
    min_amount: float = 5000,    # æœ€ä½æˆäº¤é¢(ä¸‡)ï¼Œè¿‡æ»¤æµåŠ¨æ€§ä¸è¶³
    top_n: int = 30,             # è¾“å‡ºTOP N
    factors_weight: dict = None, # è‡ªå®šä¹‰å› å­æƒé‡
) -> pd.DataFrame:
    """
    å¤šå› å­é‡åŒ–é€‰è‚¡å¼•æ“
    æµç¨‹:
    1. å…¨å¸‚åœºå¿«ç…§ â†’ åŸºç¡€è¿‡æ»¤ (ST/æ–°è‚¡/æµåŠ¨æ€§)
    2. æ‰¹é‡è·å–æ—¥çº¿ â†’ è®¡ç®—æŠ€æœ¯å› å­
    3. æ‰¹é‡è·å–èµ„é‡‘æµå‘ â†’ è®¡ç®—èµ„é‡‘å› å­
    4. å¤šå› å­åŠ æƒæ‰“åˆ† â†’ æ’åè¾“å‡º TOP N
    """
    if factors_weight is None:
        factors_weight = {
            "åŠ¨é‡_20æ—¥": 0.15,
            "é‡æ¯”_5/20": 0.15,
            "MACDé‡‘å‰": 0.10,
            "å‡çº¿å¤šå¤´": 0.10,
            "RSI_14": 0.10,
            "ä¸»åŠ›å‡€æµå…¥_5æ—¥": 0.20,
            "ä¸»åŠ›è¿ç»­æµå…¥å¤©æ•°": 0.10,
            "20æ—¥æ–°é«˜": 0.10,
        }

    # Step 1: å…¨å¸‚åœºå¿«ç…§
    snapshot = get_market_snapshot()
    if snapshot is None or snapshot.empty:
        logger.warning("å…¨å¸‚åœºå¿«ç…§ä¸ºç©º")
        return pd.DataFrame()

    # åˆ¤æ–­æ•°æ®æº (Tushare vs AKShare åˆ—åä¸åŒ)
    is_tushare = "ts_code" in snapshot.columns

    if is_tushare:
        # è¿‡æ»¤ ST + æ–°è‚¡ + æµåŠ¨æ€§
        if "name" in snapshot.columns:
            snapshot = snapshot[~snapshot["name"].str.contains("ST|é€€", na=False)]
        snapshot = snapshot[snapshot["amount"] >= min_amount / 10]  # Tushare amount åƒå…ƒ
        # è¿‡æ»¤åŒ—äº¤æ‰€ (8å¼€å¤´)
        snapshot = snapshot[~snapshot["ts_code"].str.startswith("8")]
        # è¿‡æ»¤æ¶¨è·Œåœ (æ— æ³•ä¹°å…¥)
        if "pct_chg" in snapshot.columns:
            snapshot = snapshot[(snapshot["pct_chg"] < 9.8) & (snapshot["pct_chg"] > -9.8)]
        candidates = snapshot["ts_code"].tolist()
    else:
        # AKShare æ ¼å¼
        if "åç§°" in snapshot.columns:
            snapshot = snapshot[~snapshot["åç§°"].str.contains("ST|é€€", na=False)]
        if "æˆäº¤é¢" in snapshot.columns:
            snapshot = snapshot[snapshot["æˆäº¤é¢"] >= min_amount * 1e4]
        if "ä»£ç " in snapshot.columns:
            snapshot = snapshot[~snapshot["ä»£ç "].astype(str).str.startswith("8")]
        if "æ¶¨è·Œå¹…" in snapshot.columns:
            snapshot = snapshot[(snapshot["æ¶¨è·Œå¹…"] < 9.8) & (snapshot["æ¶¨è·Œå¹…"] > -9.8)]
        # è½¬æ¢ä»£ç æ ¼å¼
        if "ä»£ç " in snapshot.columns:
            def _to_ts_code(code):
                code = str(code).zfill(6)
                if code.startswith("6"):
                    return f"{code}.SH"
                else:
                    return f"{code}.SZ"
            snapshot["ts_code"] = snapshot["ä»£ç "].apply(_to_ts_code)
            candidates = snapshot["ts_code"].tolist()
        else:
            candidates = []

    if not candidates:
        logger.warning("æ— å€™é€‰è‚¡ç¥¨")
        return pd.DataFrame()

    # å–æˆäº¤é¢ TOP 200 (é¿å…APIè¿‡è½½)
    if is_tushare:
        snapshot_sorted = snapshot.sort_values("amount", ascending=False)
    else:
        snapshot_sorted = snapshot.sort_values("æˆäº¤é¢", ascending=False)
    candidates = snapshot_sorted.head(200)["ts_code"].tolist()

    logger.info(f"[é€‰è‚¡] å€™é€‰æ± : {len(candidates)} åª")

    # Step 2: æ‰¹é‡æ—¥çº¿ + æŠ€æœ¯å› å­
    daily_data = get_multi_stock_daily(candidates[:50], days=80)

    # Step 3: å…¨å¸‚åœºèµ„é‡‘æµå‘
    mf_data = get_market_moneyflow()

    # Step 4: é€åªè®¡ç®—å› å­å¹¶æ‰“åˆ†
    scored = []
    for ts_code in candidates[:50]:
        row = {"ts_code": ts_code}

        # åç§°
        match = snapshot[snapshot["ts_code"] == ts_code]
        if not match.empty:
            if is_tushare:
                row["åç§°"] = match.iloc[0].get("name", "")
                row["è¡Œä¸š"] = match.iloc[0].get("industry", "")
                row["æ¶¨è·Œå¹…"] = float(match.iloc[0].get("pct_chg", 0))
                row["æˆäº¤é¢ä¸‡"] = round(float(match.iloc[0].get("amount", 0)) / 10, 0)
            else:
                row["åç§°"] = match.iloc[0].get("åç§°", "")
                row["è¡Œä¸š"] = ""
                row["æ¶¨è·Œå¹…"] = float(match.iloc[0].get("æ¶¨è·Œå¹…", 0))
                row["æˆäº¤é¢ä¸‡"] = round(float(match.iloc[0].get("æˆäº¤é¢", 0)) / 1e4, 0)

        # æŠ€æœ¯å› å­
        tech_factors = {}
        if ts_code in daily_data:
            tech_factors = calc_technical_factors(daily_data[ts_code])

        # èµ„é‡‘å› å­
        money_factors = {}
        if mf_data is not None and not mf_data.empty:
            stock_mf = mf_data[mf_data["ts_code"] == ts_code]
            if not stock_mf.empty:
                money_factors = calc_moneyflow_factors(stock_mf)

        all_factors = {**tech_factors, **money_factors}
        row.update(all_factors)

        # ç»¼åˆæ‰“åˆ†
        score = 0
        for factor_name, weight in factors_weight.items():
            val = all_factors.get(factor_name, 0)
            if factor_name == "åŠ¨é‡_20æ—¥":
                # æ­£åŠ¨é‡å¾—åˆ†é«˜, æ ‡å‡†åŒ–åˆ°0-100
                score += weight * min(max((val + 10) / 30 * 100, 0), 100)
            elif factor_name == "é‡æ¯”_5/20":
                score += weight * min(max(val * 50, 0), 100)
            elif factor_name in ("MACDé‡‘å‰", "å‡çº¿å¤šå¤´", "20æ—¥æ–°é«˜"):
                score += weight * (val * 100)
            elif factor_name == "RSI_14":
                # RSI 50-80 å¾—åˆ†é«˜
                if 50 <= val <= 80:
                    score += weight * 100
                elif 40 <= val < 50:
                    score += weight * 60
                elif val > 80:
                    score += weight * 30  # è¶…ä¹°æ‰£åˆ†
                else:
                    score += weight * 20
            elif factor_name == "ä¸»åŠ›å‡€æµå…¥_5æ—¥":
                score += weight * min(max((val + 5000) / 10000 * 100, 0), 100)
            elif factor_name == "ä¸»åŠ›è¿ç»­æµå…¥å¤©æ•°":
                score += weight * min(val * 20, 100)
            else:
                score += weight * min(max(val, 0), 100)

        row["ç»¼åˆå¾—åˆ†"] = round(score, 1)
        scored.append(row)

    if not scored:
        return pd.DataFrame()

    result = pd.DataFrame(scored)
    result = result.sort_values("ç»¼åˆå¾—åˆ†", ascending=False).head(top_n).reset_index(drop=True)
    result.index = result.index + 1  # æ’åä»1å¼€å§‹
    return result


# å…¼å®¹æ—§æ¥å£
def get_cls_telegraph(count: int = 50) -> list:
    return get_all_news(tushare_count=max(count, 80))