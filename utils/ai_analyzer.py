"""
AI åˆ†ææ¨¡å— V3 - å¯»æ˜Ÿæƒ…æŠ¥ä¸­å¿ƒ
================================================================
æ ¸å¿ƒå‡çº§:
  1. FOF CIO ä¸“ç”¨ System Prompt â€” å¼ºåˆ¶è¾“å‡ºå¤§ç±»é…ç½®+ç­–ç•¥æƒé‡+è¡Œä¸š+ETF
  2. ä¸‰é˜¶æ®µåˆ†ææµæ°´çº¿ â€” å®è§‚å®šæ€§ â†’ ä¸»çº¿æç‚¼ â†’ é…ç½®è¾“å‡º
  3. æ‰¹é‡èµ„è®¯åˆ†æ â€” é€æ ‡é¢˜+æ‘˜è¦, å¢å¼º JSON è§£æ
  4. å•æ¡æ·±åº¦åˆ†æ â€” FOF è§†è§’
  5. æˆæœ¬æ§åˆ¶ â€” åˆ†å±‚å¤„ç†, æ ¸å¿ƒæ–°é—»æ·±åº¦åˆ†æ, å¿«è®¯ä»…ç»Ÿè®¡æƒ…ç»ª
================================================================
"""
import json
import re
import logging
import streamlit as st
from datetime import datetime

logger = logging.getLogger("xunxing")


# ============================================================
# API åŸºç¡€
# ============================================================
def _get_api_key() -> str:
    try:
        key = st.secrets.get("DEEPSEEK_API_KEY", "")
        if key and not key.startswith("sk-xxxx"):
            return key
    except Exception:
        pass
    return ""


def _call_deepseek(prompt: str, system: str = "", temperature: float = 0.3,
                   max_tokens: int = 4000) -> str:
    """è°ƒç”¨ DeepSeek V3"""
    api_key = _get_api_key()
    if not api_key:
        return ""
    try:
        from openai import OpenAI
        client = OpenAI(api_key=api_key, base_url="https://api.deepseek.com")
        messages = []
        if system:
            messages.append({"role": "system", "content": system})
        messages.append({"role": "user", "content": prompt})
        response = client.chat.completions.create(
            model="deepseek-chat",
            messages=messages,
            temperature=temperature,
            max_tokens=max_tokens,
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        logger.error(f"DeepSeek API è°ƒç”¨å¤±è´¥: {e}")
        return f"[AIè°ƒç”¨å¤±è´¥: {e}]"


# ============================================================
# â˜… FOF CIO System Prompt (æ ¸å¿ƒ)
# ============================================================
FOF_CIO_SYSTEM = """ä½ æ˜¯ã€Œå¯»æ˜Ÿèµ„äº§é…ç½®å…¬å¸ã€çš„é¦–å¸­æŠ•èµ„å®˜ï¼ˆCIOï¼‰å…¼AIæŠ•ç ”ä¸­æ¢ã€‚

ã€ä½ çš„æ ¸å¿ƒèº«ä»½ã€‘
- ä¸“ä¸š FOFï¼ˆåŸºé‡‘ä¸­çš„åŸºé‡‘ï¼‰ç®¡ç†äººï¼Œç®¡ç†å¤šç­–ç•¥ç»„åˆ
- å†³ç­–é£æ ¼: è‡ªä¸Šè€Œä¸‹ï¼Œå¼ºè°ƒèƒœç‡ä¸ç›ˆäºæ¯”ï¼Œç»™å‡ºæ˜ç¡®å¯æ‰§è¡Œå»ºè®®
- ç»ä¸è¯´ç©ºè¯å¥—è¯ï¼Œæ¯ä¸ªåˆ¤æ–­å¿…é¡»æœ‰æ•°æ®ä¾æ®

ã€å¤§ç±»èµ„äº§é…ç½®æ¡†æ¶ã€‘
åŸºäºç¾æ—æ—¶é’Ÿ + ä¸­å›½ç‰¹è‰²ä¿®æ­£ï¼ˆæ”¿ç­–å‘¨æœŸæƒé‡æ›´é«˜ï¼‰:
- å¤è‹æœŸ â†’ è¶…é…è‚¡ç¥¨ã€æ ‡é…å•†å“ã€ä½é…å€ºåˆ¸
- è¿‡çƒ­æœŸ â†’ è¶…é…å•†å“ã€æ ‡é…è‚¡ç¥¨ã€ä½é…å€ºåˆ¸
- æ»èƒ€æœŸ â†’ è¶…é…ç°é‡‘ã€æ ‡é…å€ºåˆ¸ã€ä½é…è‚¡ç¥¨
- è¡°é€€æœŸ â†’ è¶…é…å€ºåˆ¸ã€æ ‡é…ç°é‡‘ã€ä½é…è‚¡ç¥¨

ã€FOF åº•å±‚ç­–ç•¥åˆ†ç±»ã€‘
1. è‚¡ç¥¨å¤šå¤´: ä¸»è§‚å¤šå¤´ã€è¡Œä¸šä¸»é¢˜åŸºé‡‘
2. é‡åŒ–æŒ‡å¢(500): ä»¥ä¸­è¯500ä¸ºåŸºå‡†çš„æŒ‡æ•°å¢å¼º
3. é‡åŒ–æŒ‡å¢(1000): ä»¥ä¸­è¯1000ä¸ºåŸºå‡†çš„æŒ‡æ•°å¢å¼º
4. é‡åŒ–ä¸­æ€§: å¸‚åœºä¸­æ€§ã€ç»Ÿè®¡å¥—åˆ©ã€Alphaç­–ç•¥
5. CTAç­–ç•¥: è¶‹åŠ¿è·Ÿè¸ªã€æˆªé¢åŠ¨é‡ã€åŸºæœ¬é¢é‡åŒ–
6. å¥—åˆ©ç­–ç•¥: æœŸç°å¥—åˆ©ã€ETFå¥—åˆ©ã€å¯è½¬å€ºå¥—åˆ©
7. å›ºæ”¶+/ç°é‡‘: çº¯å€ºã€è½¬å€ºå¢å¼ºã€è´§å¸ç®¡ç†

ã€é…ç½®æ¯”ä¾‹çºªå¾‹ã€‘
- æ‰€æœ‰é…ç½®å»ºè®®å¿…é¡»ç»™å‡ºå…·ä½“ç™¾åˆ†æ¯”
- ä¸ƒé¡¹ç­–ç•¥æƒé‡åˆè®¡ = 100%
- å¤§ç±»èµ„äº§å››é¡¹æƒé‡åˆè®¡ = 100%
- æ¯é¡¹é™„æ–¹å‘ç®­å¤´(â†‘å¢é…/â†“å‡é…/â†’ç»´æŒ)å’Œä¸€å¥è¯ç†ç”±
- è¡Œä¸šæ¨è: TOP3 çœ‹å¥½ + TOP3 å›é¿
- ETFæ¨è: 3-5åªå«å…·ä½“ä»£ç 

ã€ç¯å¢ƒ-ç­–ç•¥é€‚é…é€»è¾‘ã€‘
- è¶‹åŠ¿å¸‚(å•è¾¹ä¸Šæ¶¨/ä¸‹è·Œ) â†’ åˆ©å¥½è‚¡ç¥¨å¤šå¤´ã€CTAè¶‹åŠ¿
- éœ‡è¡å¸‚(çª„å¹…æ³¢åŠ¨) â†’ åˆ©å¥½é‡åŒ–ä¸­æ€§ã€å¥—åˆ©ç­–ç•¥
- é«˜æ³¢åŠ¨+åˆ†åŒ– â†’ åˆ©å¥½é‡åŒ–æŒ‡å¢(è¶…é¢æ¥æºäºæˆªé¢åˆ†åŒ–)
- ä½æ³¢åŠ¨+ç¼©é‡ â†’ CTAæ‰¿å‹ã€ä¸­æ€§è¶…é¢å‹ç¼©
- å°ç›˜æ´»è·ƒ â†’ 1000æŒ‡å¢ä¼˜äº500æŒ‡å¢
- å•†å“è¶‹åŠ¿æ˜ç¡® â†’ CTAåŠ é…"""


# ============================================================
# 1. èµ„è®¯æ‰¹é‡åˆ†æ (ä¼˜åŒ–: é€æ ‡é¢˜+æ‘˜è¦)
# ============================================================
def analyze_news_batch(news_list: list) -> list:
    """æ‰¹é‡åˆ†æèµ„è®¯ â€” é€æ ‡é¢˜+å†…å®¹æ‘˜è¦, å¢å¼ºæ‰¹æ¬¡å¤§å°"""
    if not news_list:
        return []
    api_key = _get_api_key()
    if not api_key:
        return _keyword_analysis(news_list)

    results = []
    batch_size = 15

    for i in range(0, len(news_list), batch_size):
        batch = news_list[i:i + batch_size]
        batch_text = ""
        for idx, item in enumerate(batch):
            title = item.get("title", "")
            content = item.get("content", "")[:80]
            src = item.get("source", "")
            batch_text += f"\n[{idx+1}] [{src}] {title}"
            if content and content != title:
                batch_text += f" | {content}"

        prompt = f"""åˆ†æä»¥ä¸‹{len(batch)}æ¡è´¢ç»èµ„è®¯ï¼Œè¿”å›JSONæ•°ç»„ã€‚
æ¯æ¡: id(åºå·), category(å®è§‚/è¡Œä¸š/å…¬å¸/æµ·å¤–/æ”¿ç­–), sentiment(-1åˆ°1), impact(1-5), sectors(ç›¸å…³è¡Œä¸šæ•°ç»„), summary(15å­—æ‘˜è¦)

èµ„è®¯:
{batch_text}

ç›´æ¥è¿”å›JSONæ•°ç»„ï¼Œä¸è¦å…¶ä»–æ–‡å­—:"""

        resp = _call_deepseek(prompt, "ä½ æ˜¯Aè‚¡é‡‘èåˆ†æå¸ˆï¼Œåªè¿”å›JSON",
                              temperature=0.1, max_tokens=2500)
        parsed = _parse_json(resp)
        if parsed:
            for item_data in parsed:
                idx = item_data.get("id", 0) - 1
                if 0 <= idx < len(batch):
                    batch[idx]["analysis"] = item_data
            results.extend(batch)
        else:
            results.extend(_keyword_analysis(batch))

    return results


# ============================================================
# 2. æ ¸å¿ƒä¸»çº¿æç‚¼
# ============================================================
def summarize_market_threads(news_list: list) -> str:
    """ä»èµ„è®¯ä¸­æç‚¼æ ¸å¿ƒæŠ•èµ„ä¸»çº¿"""
    api_key = _get_api_key()
    if not api_key or not news_list:
        return "âš ï¸ æœªé…ç½® API å¯†é’¥æˆ–æ— èµ„è®¯æ•°æ®ã€‚"

    text_blocks = [f"- [{n.get('source','')}] {n.get('title','')} {n.get('content','')[:60]}"
                   for n in news_list]
    news_text = "\n".join(text_blocks[:80])

    system = """ä½ æ˜¯å¯»æ˜Ÿèµ„äº§é…ç½®å…¬å¸çš„ CIOã€‚ä»ç¢ç‰‡åŒ–èµ„è®¯ä¸­æç‚¼å½“å‰å¸‚åœºæœ€å…·çˆ†å‘åŠ›çš„æŠ•èµ„ä¸»çº¿ã€‚
ç»ä¸è¦æµæ°´è´¦ç½—åˆ—æ–°é—»ï¼Œå¿…é¡»å¯»æ‰¾ç¾¤ä½“æ€§ã€è¡Œä¸šæ€§æˆ–å®è§‚çº§åˆ«çš„äº‹ä»¶å…±æŒ¯ã€‚"""

    prompt = f"""åŸºäºä»¥ä¸‹ {len(news_list)} æ¡å¸‚åœºèµ„è®¯ï¼Œæç‚¼å½“å‰æœ€æ ¸å¿ƒçš„ 3 æ¡æŠ•èµ„ä¸»çº¿ã€‚

ã€æ ¼å¼è¦æ±‚ã€‘
### ğŸ”¥ å¸‚åœºæ ¸å¿ƒä¸»çº¿æç‚¼
1. **[ä¸»çº¿åç§°]**ï¼š(å‚¬åŒ–å‰‚äº‹ä»¶)
   - **FOFé…ç½®æ€è·¯**ï¼š(ETFæˆ–ç­–ç•¥é…ç½®å»ºè®®)
2. ...
3. ...

ã€è¾“å…¥èµ„è®¯ã€‘
{news_text}"""
    return _call_deepseek(prompt, system, temperature=0.3, max_tokens=2500)


# ============================================================
# 3. â˜… FOF CIO æ—¥åº¦é…ç½®æŠ¥å‘Š (æ ¸å¿ƒå‡çº§)
# ============================================================
def generate_daily_report(market_text: str, news_text: str) -> str:
    """ç”Ÿæˆ FOF CIO æ—¥åº¦é…ç½®æŠ¥å‘Š â€” å¼ºåˆ¶ç»“æ„åŒ–è¾“å‡º"""
    api_key = _get_api_key()
    if not api_key:
        return "âš ï¸ æœªé…ç½® API Keyã€‚"

    today_str = datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥')

    prompt = f"""åŸºäºä»¥ä¸‹å®¢è§‚æ•°æ®ï¼Œç”Ÿæˆ {today_str} å¯»æ˜Ÿ FOF CIO æ—¥åº¦é…ç½®æŠ¥å‘Šã€‚

ã€è¾“å…¥æ•°æ®ã€‘
{market_text}

{news_text}

ã€å¼ºåˆ¶è¾“å‡ºç»“æ„ â€” æ¯ä¸ªéƒ¨åˆ†å¿…é¡»å¡«å†™å…·ä½“æ•°å€¼ï¼Œä¸å¯çœç•¥ä»»ä½•ä¸€é¡¹ã€‘

### ğŸ”­ ä¸€ã€å®è§‚ç¯å¢ƒä¸ç»æµå‘¨æœŸåˆ¤æ–­
- å½“å‰æ‰€å¤„å‘¨æœŸè±¡é™: [å¤è‹ / è¿‡çƒ­ / æ»èƒ€ / è¡°é€€] (å››é€‰ä¸€)
- æ ¸å¿ƒä¾æ® (ä¸è¶…è¿‡3å¥è¯ï¼Œå¼•ç”¨å…·ä½“æ•°æ®)
- è¾ƒä¸ŠæœŸå˜åŒ–: [ç»´æŒ / ä»__å‘__è½¬å‘]

### ğŸ§­ äºŒã€å¤§ç±»èµ„äº§é…ç½®ä¿¡å·
(å››é¡¹åˆè®¡å¿…é¡» = 100%)
| èµ„äº§ç±»åˆ« | å»ºè®®æƒé‡ | æ–¹å‘ | æ ¸å¿ƒé€»è¾‘ |
|---------|---------|------|---------|
| æƒç›Š | __% | â†‘/â†“/â†’ | ... |
| å›ºæ”¶ | __% | â†‘/â†“/â†’ | ... |
| å•†å“ | __% | â†‘/â†“/â†’ | ... |
| ç°é‡‘ | __% | â†‘/â†“/â†’ | ... |

### ğŸ§© ä¸‰ã€FOF åº•å±‚ç­–ç•¥é…ç½®å»ºè®®
(ä¸ƒé¡¹åˆè®¡å¿…é¡» = 100%)
| ç­–ç•¥ç±»å‹ | å»ºè®®æƒé‡ | æ–¹å‘ | é€‚é…ç¯å¢ƒä¸ç†ç”± |
|---------|---------|------|-------------|
| è‚¡ç¥¨å¤šå¤´ | __% | â†‘/â†“/â†’ | ... |
| é‡åŒ–æŒ‡å¢(500) | __% | â†‘/â†“/â†’ | ... |
| é‡åŒ–æŒ‡å¢(1000) | __% | â†‘/â†“/â†’ | ... |
| é‡åŒ–ä¸­æ€§ | __% | â†‘/â†“/â†’ | ... |
| CTAç­–ç•¥ | __% | â†‘/â†“/â†’ | ... |
| å¥—åˆ©ç­–ç•¥ | __% | â†‘/â†“/â†’ | ... |
| å›ºæ”¶+/ç°é‡‘ | __% | â†‘/â†“/â†’ | ... |

### ğŸ­ å››ã€å¸‚åœºé£æ ¼ä¸è¡Œä¸šç ”åˆ¤
- å¤§å°ç›˜é£æ ¼: å[å¤§ç›˜/å°ç›˜]ï¼Œç†ç”±ä¸€å¥è¯
- æˆé•¿ä»·å€¼é£æ ¼: å[æˆé•¿/ä»·å€¼]ï¼Œç†ç”±ä¸€å¥è¯
- **çœ‹å¥½è¡Œä¸š TOP3**:
  1. __: (å‚¬åŒ–å‰‚ä¸€å¥è¯)
  2. __: (å‚¬åŒ–å‰‚ä¸€å¥è¯)
  3. __: (å‚¬åŒ–å‰‚ä¸€å¥è¯)
- **å›é¿è¡Œä¸š TOP3**:
  1. __: (é£é™©ä¸€å¥è¯)
  2. __: (é£é™©ä¸€å¥è¯)
  3. __: (é£é™©ä¸€å¥è¯)

### ğŸ¯ äº”ã€æˆ˜æœ¯å·¥å…·ç®±
**ETFæ¨è** (3-5åª):
| ETFåç§° | ä»£ç  | é…ç½®é€»è¾‘ |
|---------|------|---------|

**ä¸ªè‚¡çº¿ç´¢** (2-3æ¡):
| æ–¹å‘/ä¸»é¢˜ | å‚¬åŒ–å‰‚ | å…³æ³¨æ ‡çš„ç±»å‹ |
|----------|--------|------------|

### ğŸ›¡ï¸ å…­ã€é£é™©é¢„è­¦ä¸å¯¹å†²é¢„æ¡ˆ
(åˆ—å‡ºæœ€é‡è¦çš„3ä¸ªé£é™©ï¼Œæ¯ä¸ªé™„å¯¹å†²å»ºè®®)
1. **é£é™©**: ... â†’ **å¯¹å†²**: ...
2. **é£é™©**: ... â†’ **å¯¹å†²**: ...
3. **é£é™©**: ... â†’ **å¯¹å†²**: ...

---
âš ï¸ å…è´£å£°æ˜: æœ¬æŠ¥å‘Šç”±AIåŸºäºå…¬å¼€æ•°æ®ç”Ÿæˆï¼Œä»…ä¾›å†…éƒ¨å‚è€ƒï¼Œä¸æ„æˆæŠ•èµ„å»ºè®®ã€‚å¸‚åœºæœ‰é£é™©ï¼ŒæŠ•èµ„éœ€è°¨æ…ã€‚"""

    return _call_deepseek(prompt, FOF_CIO_SYSTEM, temperature=0.35, max_tokens=5000)


# ============================================================
# 4. å•æ¡æ·±åº¦åˆ†æ (FOFè§†è§’)
# ============================================================
def analyze_single_news(text: str) -> str:
    """ä»¥ FOF CIO è§†è§’æ·±åº¦åˆ†æå•æ¡èµ„è®¯"""
    api_key = _get_api_key()
    if not api_key:
        return "è¯·å…ˆé…ç½® DeepSeek API Key"
    prompt = f"""ä»¥ FOF CIO è§†è§’æ·±åº¦åˆ†æä»¥ä¸‹èµ„è®¯:
{text}

è¯·ä¾æ¬¡å›ç­”:
1. **äº‹ä»¶å®šæ€§**: ä»€ä¹ˆçº§åˆ«çš„äº‹ä»¶ï¼Ÿ(å®è§‚/è¡Œä¸š/å…¬å¸) é‡è¦æ€§1-5æ˜Ÿ
2. **å½±å“èŒƒå›´**: å“ªäº›èµ„äº§ç±»åˆ«/è¡Œä¸šæ¿å—å—å½±å“ï¼Ÿæ–¹å‘å¦‚ä½•ï¼Ÿ
3. **å¯¹FOFå„ç­–ç•¥çš„å½±å“**:
   - è‚¡ç¥¨å¤šå¤´: å½±å“__
   - é‡åŒ–æŒ‡å¢: å½±å“__
   - é‡åŒ–ä¸­æ€§: å½±å“__
   - CTA: å½±å“__
   - å¥—åˆ©: å½±å“__
4. **æŒç»­æ€§åˆ¤æ–­**: çŸ­æœŸè„‰å†²(1-3å¤©) / ä¸­æœŸè¶‹åŠ¿(1-3æœˆ) / é•¿æœŸç»“æ„æ€§
5. **åº”å¯¹å»ºè®®**: å…·ä½“çš„FOFé…ç½®è°ƒæ•´åŠ¨ä½œ"""
    return _call_deepseek(prompt, FOF_CIO_SYSTEM, temperature=0.3, max_tokens=2000)


# ============================================================
# è¾…åŠ©å‡½æ•°
# ============================================================
def _parse_json(text: str):
    """å¢å¼ºç‰ˆ JSON è§£æ â€” å®¹å¿ AI è¾“å‡ºæ ¼å¼åå·®"""
    if not text or text.startswith("[AIè°ƒç”¨å¤±è´¥"):
        return None
    text = text.strip()
    # ç§»é™¤ markdown ä»£ç å—åŒ…è£¹
    text = re.sub(r'^```(?:json)?\s*', '', text)
    text = re.sub(r'\s*```$', '', text)
    text = text.strip()
    # ç›´æ¥è§£æ
    try:
        r = json.loads(text)
        return r if isinstance(r, list) else [r]
    except Exception:
        pass
    # æå–æ•°ç»„éƒ¨åˆ†
    match = re.search(r'\[.*\]', text, re.DOTALL)
    if match:
        try:
            return json.loads(match.group())
        except Exception:
            pass
    # å°è¯•ä¿®å¤: ç§»é™¤å°¾éƒ¨é€—å·
    try:
        cleaned = re.sub(r',\s*([}\]])', r'\1', text)
        match2 = re.search(r'\[.*\]', cleaned, re.DOTALL)
        if match2:
            return json.loads(match2.group())
    except Exception:
        pass
    return None


def _keyword_analysis(news_list: list) -> list:
    """å…³é”®è¯é™çº§åˆ†æ (æ—  API æ—¶ä½¿ç”¨)"""
    pos_words = ["åˆ©å¥½", "ä¸Šæ¶¨", "å¢é•¿", "çªç ´", "è¶…é¢„æœŸ", "åˆ›æ–°é«˜", "æ”¯æŒ", "æ‰©å¤§", "å›æš–", "åŠ é€Ÿ"]
    neg_words = ["åˆ©ç©º", "ä¸‹è·Œ", "ä¸‹é™", "ä½äºé¢„æœŸ", "æ”¶ç¼©", "æš´è·Œ", "æ”¶ç´§", "é£é™©", "å‡æŒ", "è¿çº¦"]
    cat_map = {
        "å®è§‚": ["GDP", "CPI", "PPI", "PMI", "å¤®è¡Œ", "é™å‡†", "é™æ¯", "åˆ©ç‡", "MLF", "ç¤¾è", "ä¸¤ä¼š", "å›½åŠ¡é™¢"],
        "æµ·å¤–": ["ç¾è”å‚¨", "ç¾å›½", "æ¬§æ´²", "ç¾è‚¡", "ç¾å€º", "ç¾å…ƒ", "å…³ç¨", "æ—¥æœ¬", "è‹±å›½"],
        "æ”¿ç­–": ["å·¥ä¿¡éƒ¨", "å‘æ”¹å§”", "è¯ç›‘ä¼š", "å›½åŠ¡é™¢", "æ”¿ç­–", "è§„åˆ’", "ç›‘ç®¡", "è´¢æ”¿éƒ¨", "é“¶ä¿ç›‘"],
        "è¡Œä¸š": ["åŠå¯¼ä½“", "èŠ¯ç‰‡", "AI", "äººå·¥æ™ºèƒ½", "æœºå™¨äºº", "æ–°èƒ½æº", "åŒ»è¯", "å†›å·¥", "æ±½è½¦", "å…‰ä¼"],
    }
    sec_map = {
        "åŠå¯¼ä½“": ["åŠå¯¼ä½“", "èŠ¯ç‰‡", "æ™¶åœ†", "å…‰åˆ»", "EDA"],
        "AI": ["AI", "äººå·¥æ™ºèƒ½", "å¤§æ¨¡å‹", "ç®—åŠ›", "æœºå™¨äºº", "GPU"],
        "æ–°èƒ½æº": ["æ–°èƒ½æº", "å…‰ä¼", "é”‚ç”µ", "å‚¨èƒ½", "é£ç”µ", "ç¢³ä¸­å’Œ"],
        "åŒ»è¯": ["åŒ»è¯", "åˆ›æ–°è¯", "GLP", "åŒ»ç–—", "CXO"],
        "æ¶ˆè´¹": ["æ¶ˆè´¹", "ç™½é…’", "é£Ÿå“", "æ—…æ¸¸", "å…ç¨", "å®¶ç”µ"],
        "é‡‘è": ["é“¶è¡Œ", "åˆ¸å•†", "ä¿é™©", "é‡‘è", "ä¿¡æ‰˜"],
        "å†›å·¥": ["å†›å·¥", "å›½é˜²", "èˆªç©º", "èˆªå¤©", "å¯¼å¼¹"],
    }
    for item in news_list:
        text = item.get("title", "") + item.get("content", "")
        category = "å…¬å¸"
        for cat, kws in cat_map.items():
            if any(k in text for k in kws):
                category = cat
                break
        pos = sum(1 for w in pos_words if w in text)
        neg = sum(1 for w in neg_words if w in text)
        sentiment = round(min(max((pos - neg) * 0.25, -1), 1), 2)
        sectors = [s for s, kws in sec_map.items() if any(k in text for k in kws)]
        item["analysis"] = {
            "category": category,
            "sentiment": sentiment,
            "impact": 3 if item.get("important") else 2,
            "sectors": sectors[:3],
            "summary": item.get("title", "")[:15],
        }
    return news_list
