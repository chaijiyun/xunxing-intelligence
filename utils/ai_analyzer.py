"""
AI åˆ†ææ¨¡å— - DeepSeek API
"""
import json
import streamlit as st
from datetime import datetime

def _get_api_key() -> str:
    try:
        key = st.secrets.get("DEEPSEEK_API_KEY", "")
        if key and not key.startswith("sk-xxxx"):
            return key
    except Exception:
        pass
    return ""

def _call_deepseek(prompt: str, system: str = "", temperature: float = 0.3, max_tokens: int = 4000) -> str:
    """è°ƒç”¨ DeepSeek V3"""
    api_key = _get_api_key()
    if not api_key:
        return ""

    try:
        from openai import OpenAI
        client = OpenAI(api_key=api_key, base_url="https://api.deepseek.com")

        messages = []
        if system:
            messages.append({"role": "system", "content": system})
        messages.append({"role": "user", "content": prompt})

        response = client.chat.completions.create(
            model="deepseek-chat",
            messages=messages,
            temperature=temperature,
            max_tokens=max_tokens,
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        return f"[AIè°ƒç”¨å¤±è´¥: {e}]"

# ============================================================
# èµ„è®¯æ‰¹é‡åˆ†æ
# ============================================================
def analyze_news_batch(news_list: list) -> list:
    """æ‰¹é‡åˆ†æèµ„è®¯"""
    if not news_list:
        return []

    api_key = _get_api_key()
    if not api_key:
        return _keyword_analysis(news_list)

    results = []
    batch_size = 10
    for i in range(0, len(news_list), batch_size):
        batch = news_list[i:i + batch_size]

        batch_text = ""
        for idx, item in enumerate(batch):
            batch_text += f"\n[{idx+1}] {item.get('title','')}\n"

        prompt = f"""åˆ†æä»¥ä¸‹{len(batch)}æ¡Aè‚¡è´¢ç»èµ„è®¯ï¼Œè¿”å›JSONæ•°ç»„ã€‚
æ¯æ¡åŒ…å«ï¼šid(åºå·), category(å®è§‚/è¡Œä¸š/å…¬å¸/æµ·å¤–/æ”¿ç­–), sentiment(-1åˆ°1), impact(1-5), sectors(ç›¸å…³è¡Œä¸šæ•°ç»„), summary(15å­—æ‘˜è¦)

èµ„è®¯ï¼š
{batch_text}

ç›´æ¥è¿”å›JSONæ•°ç»„ï¼Œä¸è¦å…¶ä»–æ–‡å­—ï¼š"""

        resp = _call_deepseek(prompt, "ä½ æ˜¯Aè‚¡é‡‘èåˆ†æå¸ˆï¼Œåªè¿”å›JSON", temperature=0.1, max_tokens=2000)

        parsed = _parse_json(resp)
        if parsed:
            for item in parsed:
                idx = item.get("id", 0) - 1
                if 0 <= idx < len(batch):
                    batch[idx]["analysis"] = item
            results.extend(batch)
        else:
            results.extend(_keyword_analysis(batch))

    return results

def _parse_json(text: str):
    if not text or text.startswith("[AIè°ƒç”¨å¤±è´¥"):
        return None
    text = text.strip().removeprefix("```json").removeprefix("```").removesuffix("```").strip()
    try:
        r = json.loads(text)
        return r if isinstance(r, list) else [r]
    except Exception:
        s, e = text.find("["), text.rfind("]") + 1
        if s >= 0 and e > s:
            try:
                return json.loads(text[s:e])
            except Exception:
                pass
    return None

def _keyword_analysis(news_list: list) -> list:
    pos_words = ["åˆ©å¥½", "ä¸Šæ¶¨", "å¢é•¿", "çªç ´", "è¶…é¢„æœŸ", "åˆ›æ–°é«˜", "æ”¯æŒ", "æ‰©å¤§"]
    neg_words = ["åˆ©ç©º", "ä¸‹è·Œ", "ä¸‹é™", "ä½äºé¢„æœŸ", "æ”¶ç¼©", "æš´è·Œ", "æ”¶ç´§", "é£é™©"]
    cat_map = {
        "å®è§‚": ["GDP", "CPI", "PPI", "PMI", "å¤®è¡Œ", "é™å‡†", "é™æ¯", "åˆ©ç‡", "MLF", "ç¤¾è", "ä¸¤ä¼š"],
        "æµ·å¤–": ["ç¾è”å‚¨", "ç¾å›½", "æ¬§æ´²", "ç¾è‚¡", "ç¾å€º", "ç¾å…ƒ", "å…³ç¨"],
        "æ”¿ç­–": ["å·¥ä¿¡éƒ¨", "å‘æ”¹å§”", "è¯ç›‘ä¼š", "å›½åŠ¡é™¢", "æ”¿ç­–", "è§„åˆ’", "ç›‘ç®¡"],
        "è¡Œä¸š": ["åŠå¯¼ä½“", "èŠ¯ç‰‡", "AI", "äººå·¥æ™ºèƒ½", "æœºå™¨äºº", "æ–°èƒ½æº", "åŒ»è¯", "å†›å·¥"],
    }
    sec_map = {
        "åŠå¯¼ä½“": ["åŠå¯¼ä½“", "èŠ¯ç‰‡", "æ™¶åœ†"],
        "AI": ["AI", "äººå·¥æ™ºèƒ½", "å¤§æ¨¡å‹", "ç®—åŠ›", "æœºå™¨äºº"],
        "æ–°èƒ½æº": ["æ–°èƒ½æº", "å…‰ä¼", "é”‚ç”µ", "å‚¨èƒ½"],
        "åŒ»è¯": ["åŒ»è¯", "åˆ›æ–°è¯", "GLP", "åŒ»ç–—"],
        "æ¶ˆè´¹": ["æ¶ˆè´¹", "ç™½é…’", "é£Ÿå“", "æ—…æ¸¸"],
        "é‡‘è": ["é“¶è¡Œ", "åˆ¸å•†", "ä¿é™©"],
    }

    for item in news_list:
        text = item.get("title", "") + item.get("content", "")
        category = "å…¬å¸"
        for cat, kws in cat_map.items():
            if any(k in text for k in kws):
                category = cat
                break

        pos = sum(1 for w in pos_words if w in text)
        neg = sum(1 for w in neg_words if w in text)
        sentiment = round(min(max((pos - neg) * 0.25, -1), 1), 2)
        sectors = [s for s, kws in sec_map.items() if any(k in text for k in kws)]

        item["analysis"] = {
            "category": category,
            "sentiment": sentiment,
            "impact": 3 if item.get("important") else 2,
            "sectors": sectors[:3],
            "summary": item.get("title", "")[:15],
        }

    return news_list

def generate_daily_report(market_text: str, news_text: str) -> str:
    api_key = _get_api_key()
    if not api_key:
        return "âš ï¸ æœªé…ç½® API Keyã€‚"

    system = """ä½ æ˜¯ã€Œå¯»æ˜Ÿèµ„äº§é…ç½®å…¬å¸ã€çš„é¦–å¸­æŠ•èµ„å®˜ï¼ˆCIOï¼‰å…¼AIæŠ•ç ”ä¸­æ¢ã€‚
    æ ¸å¿ƒä»»åŠ¡ï¼šä¸ºä¸“ä¸š FOF ç®¡ç†äººæä¾›è‡ªä¸Šè€Œä¸‹çš„èµ„äº§é…ç½®å†³ç­–ã€‚å¼ºè°ƒèƒœç‡ä¸ç›ˆäºæ¯”ï¼Œç»™å‡ºæ˜ç¡®å»ºè®®ã€‚"""

    prompt = f"""åŸºäºä»¥ä¸‹å®¢è§‚æ•°æ®ï¼Œç”Ÿæˆ {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥')} å¯»æ˜Ÿå¸‚åœºæ—¥æŠ¥ã€‚

ã€è¾“å…¥æ•°æ®ã€‘
{market_text}
{news_text}

ã€æŠ¥å‘Šæ¡†æ¶è¦æ±‚ã€‘
### ğŸ”­ ä¸€ã€ å¸‚åœºå¼‚åŠ¨ä¸å®è§‚å®šè°ƒ
### ğŸ§­ äºŒã€ å¯»æ˜Ÿå¤§ç±»èµ„äº§é…ç½®æ—¶é’Ÿ
### ğŸ§© ä¸‰ã€ FOF åº•å±‚ç­–ç•¥è°ƒä»“æŒ‡å—
### ğŸ­ å››ã€ Aè‚¡ç»“æ„ä¸é£æ ¼ç ”åˆ¤
### ğŸ¯ äº”ã€ å¯»æ˜Ÿæˆ˜æœ¯å·¥å…·ç®± (ETFä¸ä¸ªè‚¡)
### ğŸ›¡ï¸ å…­ã€ å°¾éƒ¨é£é™©ä¸å¯¹å†²é¢„æ¡ˆ
"""
    return _call_deepseek(prompt, system, temperature=0.4, max_tokens=4000)

def analyze_single_news(text: str) -> str:
    api_key = _get_api_key()
    if not api_key:
        return "è¯·å…ˆé…ç½® DeepSeek API Key"

    prompt = f"""æ·±åº¦åˆ†æä»¥ä¸‹èµ„è®¯ï¼š\n{text}\n\n1.äº‹ä»¶å®šæ€§ 2.å½±å“èŒƒå›´ 3.å¯¹èµ„äº§å½±å“ 4.æŒç»­æ€§ 5.åº”å¯¹å»ºè®®"""
    return _call_deepseek(prompt, temperature=0.3, max_tokens=2000)

# ============================================================
# å…¨å±€å¤§åŠ¿æç‚¼ (æ ¸å¿ƒä¸»çº¿æå–)
# ============================================================
def summarize_market_threads(news_list: list) -> str:
    """æå–æ•°ç™¾æ¡æ–°é—»ä¸­çš„æ ¸å¿ƒæŠ•èµ„ä¸»çº¿"""
    api_key = _get_api_key()
    if not api_key or not news_list:
        return "âš ï¸ æœªé…ç½® API å¯†é’¥æˆ–æ— èµ„è®¯æ•°æ®ã€‚"

    # å°†æ‰€æœ‰æ–°é—»æµ“ç¼©ä¸ºçº¯æ–‡æœ¬
    text_blocks = [f"- {n.get('title','')} {n.get('content','')[:50]}" for n in news_list]
    news_text = "\n".join(text_blocks)

    system = """ä½ æ˜¯å¯»æ˜Ÿèµ„äº§é…ç½®å…¬å¸çš„ CIOã€‚ä½ çš„ä»»åŠ¡æ˜¯ä»ä¸€å †ç¢ç‰‡åŒ–èµ„è®¯ä¸­ï¼Œæç‚¼å‡ºå½“å‰å¸‚åœºæœ€å…·çˆ†å‘åŠ›çš„æŠ•èµ„ä¸»çº¿ã€‚
    ç»ä¸è¦æµæ°´è´¦ç½—åˆ—æ–°é—»ï¼Œå¿…é¡»å¯»æ‰¾ç¾¤ä½“æ€§ã€è¡Œä¸šæ€§æˆ–å®è§‚çº§åˆ«çš„äº‹ä»¶å…±æŒ¯ã€‚"""

    prompt = f"""åŸºäºä»¥ä¸‹ {len(news_list)} æ¡æœ€æ–°æ¸…æ´—åçš„å¸‚åœºèµ„è®¯ï¼Œä¸ºä½ æç‚¼å‡ºå½“å‰å¸‚åœºæœ€æ ¸å¿ƒçš„ 3 æ¡æŠ•èµ„ä¸»çº¿æˆ–å®è§‚å¼‚åŠ¨ã€‚
    
    ã€æ ¼å¼è¦æ±‚ã€‘ï¼ˆç›´æ¥è¾“å‡º Markdown æ ¼å¼ï¼‰
    ### ğŸ”¥ å¸‚åœºæ ¸å¿ƒä¸»çº¿æç‚¼
    1. **[ä¸»çº¿åç§°/æ¿å—]**ï¼š(ä¸€å¥è¯è§£é‡ŠèƒŒåçš„å‚¬åŒ–å‰‚äº‹ä»¶)
       - **é…ç½®æ€è·¯**ï¼š(ä»FOFç­–ç•¥æˆ–ETFé…ç½®è§’åº¦ç»™å‡ºåº”å¯¹å»ºè®®)
    2. ... (ä»¥æ­¤ç±»æ¨ï¼Œå¿…é¡»å†™æ»¡ 3 æ¡)

    ã€è¾“å…¥èµ„è®¯ã€‘
    {news_text}
    """
    # æ‰©å¤§ max_tokens é˜²æ­¢è¢«æˆªæ–­
    return _call_deepseek(prompt, system, temperature=0.3, max_tokens=2500)